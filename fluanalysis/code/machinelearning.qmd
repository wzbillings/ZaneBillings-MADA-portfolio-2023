---
title: "Flu analysis: Machine learning"
output:
  html_document:
    toc: FALSE
---

```{r setup, include = FALSE}
# Declare dependencies, I was feeling super lazy today but should have been
# much more explicit than this
box::use(
	tidymodels,
	tidyverse,
	ggplot2[...],
	rpart,
	rpart.plot,
	glmnet,
	ranger
)

if (interactive()) {options(tidymodels.dark = TRUE)}
```

The first thing we need to do for this exercise (the final one in our
flu analysis series) is a bit more data wrangling. We'll deal with
feature selection, rank deficiency, and near-zero variance predictors. All of
the issues we want to solve are explained well on the website, so I'll let
y'all [check that](https://andreashandel.github.io/MADAcourse/Assessment_ML_Models_1.html) for explanations about that stuff. I'll go ahead and
clean the data so you can see how I would deal with these issues.

```{r}
# First load in the data from last time
dat_orig <- readr::read_rds(here::here("fluanalysis", "data", "clean-data.Rds"))

# Next we'll do the data processing that we need
dat <-
	dat_orig |>
	# Convert to tibble so it prints nicer
	tibble::as_tibble() |>
	# Remove the unwanted variables -- we don't need the YN versions of these
	# variables cause the ordinal versions contain more information.
	# I already did the ordinal coding in a previous script, you can check
	# the previous solutions for how to do that part.
	dplyr::select(-c(CoughYN, CoughYN2, MyalgiaYN, WeaknessYN)) |>
	# Remove binary variables with < 50 events
	dplyr::select(
		# Remove all UNORDERED factor variables with less than 50 yes
		# We can write an anonymous function (the \(x) declares this) that
		# will select all variables that are factors, not ordered, and have
		# less than 50 Yes entries. Then the - sign negates that so we drop
		# those variables instead of keeping them.
		-where(\(x) (sum(x == "Yes") < 50) & is.factor(x) & !is.ordered(x)),
	)

dplyr::glimpse(dat)
```

You can see that we have 730 rows and 26 columns like we are supposed to!
Now our data is ready to go. The first thing we need to do is
"spend our data budget."

# Data setup

First we'll split the dataset into training and testing sets.

```{r}
# Now split the data
set.seed(123)
dat_split <-
	dat |>
	rsample::initial_split(
		prop = 0.7,
		strata = BodyTemp
	)
dat_train <- rsample::training(dat_split)
dat_test <- rsample::testing(dat_split)
```

Now we'll create a set of 5x5 cross validation folds.

```{r}
dat_cv <-
	dat |>
	rsample::vfold_cv(
		v = 5,
		repeats = 5,
		strata = BodyTemp
	)
```

Finally we'll create our recipe for fitting the models. For this assignment,
our continuous outcome will be `BodyTemp`.

```{r}
# recipe for BodyTemp
rec_bodytemp <-
	# Create the recipe with the formula specifying the outcome
	dat_train |>
	recipes::recipe(formula = BodyTemp ~ .) |>
	# Convert the ordinal variables into integers. This may not be the best way
	# to do this, but it is easy and sufficiently handling ordinal predictors
	# is actually a very complicated topic.
	recipes::step_ordinalscore(Cough, Myalgia, Weakness) |>
	# Convert nominal factor variables into 0/1 dummy variables (AKA indicators)
	recipes::step_dummy(recipes::all_nominal_predictors())
```

# Null model performance

For our continuous outcome, we need to fit a **null model** in order to
assess baseline performance. For our categorical model, most accuracy metrics
are interpretable (like accuracy and ROC AUC), but for continuous outcomes,
most metrics are in response units and there is no "upper bound," so assessing
the performance on a null model is needed to understand how good our
models actually are.

Instead of just doing the RMSE (which you can get with `yardstick::rmse()`),
I thought it would be interesting to use a `metric_set()` here: a collection
of `yardstick` metrics that you can get all at the same time. So first we'll
define the metric set we want to use.

```{r}
metrics <-
	yardstick::metric_set(
		# RMSE: the "standard" metric
		yardstick::rmse,
		# MAE: outliers are not as influential
		yardstick::mae
		# That's all for now but you can add others if you like.
	)
```


```{r}
# First create a parsnip model specification
null_spec <-
	parsnip::null_model() |>
	# Our outcome is continuous, so this is a regression problem
	parsnip::set_mode("regression") |>
	# The only choice for the engine is parsnip -- you don't technically need to
	# set this, but sometimes if you don't, it breaks
	parsnip::set_engine("parsnip")

# Now we make a workflow
null_wf <-
	workflows::workflow() |>
	workflows::add_recipe(rec_bodytemp) |>
	workflows::add_model(null_spec)

# Now we fit the workflow and get predictions
null_wf_train <-
	null_wf |>
	parsnip::fit(data = dat_train)

null_metrics <-
	null_wf_train |>
	broom::augment(new_data = dat_train) |>
	metrics(truth = BodyTemp, estimate = .pred)

null_metrics
```

Now we have a baseline RMSE and
MAE that we can use for comparison to our fitted models.

# Model tuning and fitting

For this exercise, we need to fit three models: a tree, a LASSO model, and
a random forest model. We'll start with the tree.

## Tree Model

For this model, I'll explain all the details for what we're doing. In subsequent
models, I'll just explain what is new. However, the first step is making a
recipe, which we already know how to do. The only part that changes here
is the model function and engine we need to set. You can find a list of
model functions and the engines/modes they accept on the [parsnip site](https://www.tidymodels.org/find/parsnip/).

If you search for "tree" in the title section of that page, you can
see that the function we need to use is `decision_tree()`. We use this in the
same place that we used `linear_reg()` before. You can see on the website
page for this function that there are multiple different engines
available. But we'll use the one that is the default and the most common
decision tree engine in R, called `rpart`. Make sure you install the
`rpart` package for this!

Note that in this recipe, there are **hyperparameters** that we have to specify!
A tidymodels `decision_tree()` has three tuning parameters (one of which
they ignore in the tutorial, and has the default chosen by `rpart`): tree
depth, cost complexity, and min $n$. Here we'll tune all three of those,
so we'll put in *placeholders* for those parameters.

```{r}
# Create the recipe
tree_spec <-
	parsnip::decision_tree(
		cost_complexity = parsnip::tune(),
		tree_depth = parsnip::tune(),
		min_n = parsnip::tune()
	) |>
	# Decision trees can be for either classification or regression so you
	# should always set the mode for this model!!
	parsnip::set_mode("regression") |>
	# Set the engine -- I recommend that you do this even though we are using
	# the default because the tidyverse/tidymodels teams sometimes like
	# to change the defaults
	parsnip::set_engine("rpart")

# Put the recipe and model together into a workflow
tree_wf <-
	workflows::workflow() |>
	workflows::add_recipe(rec_bodytemp) |>
	workflows::add_model(tree_spec)
```

Now we'll do something new -- we need to **tune** the model! If you try to
fit the model now, you'll get some issues because of those tuning placeholders
we put in before. Now we'll fit our model with many different combinations
of these tuning parameters (aka hyperparameters, I keep switching back and
forth but they mean the same thing) to our cross-validation folds.

We are going to tune by **grid evaluation**, which is a simple and easy, but
relatively inefficient, way to select hyperparameters. We just need to define
a grid of values that each of our parameters can take on. Figuring out
what values these parameters are allowed to take and what a good range to
choose is actually quite difficult, so I recommend looking at the
defaults and then updating as you like.

```{r}
# Extract the parameters from the model
tree_parms <-	hardhat::extract_parameter_set_dials(tree_spec)

# Build a regularly sampled grid
dials::grid_regular(tree_parms)
```

OK so from this sample you can kind of see what the values should look like.
Since we now know what to expect, I'm going to build a grid myself. I encourage
you to test the other options of `grid_regular()` and also look at alternatives
like `grid_latin_hypercube()`. This kind of stuff is not very intuitive so
don't feel bad about using the defaults. The more you work on this,
the more you get a feeling for what kind of values are appropriate. Of course
I don't know anything about this example so we'll see how well my model works.
Remember that **the more sets of parameters you test, the longer tuning will
take**. If you need to do a lot of tuning in order to get a well-fitting model,
you should look into other tuning methods.

```{r}
tree_grid <-
	tidyr::expand_grid(
		cost_complexity = 10 ^ c(-10, -5, -3, -1),
		tree_depth = c(2, 3, 5, 10),
		min_n = c(2, 20, 50)
	)

dplyr::glimpse(tree_grid)
```

This doesn't look like a lot of values to test but you can see that there are
48 combinations. Remember that we are doing 5x5 cross validation, and you
can see that we are already up to $48 \times 25$ model evaluations
so you can see how this can take a long time!

Ok, so now we need to invoke the tuning routine.

```{r}
#| message: false
#| warning: false
#| cache: true
#| cache-lazy: false
tree_res <-
	tree_wf |>
	tune::tune_grid(
		# Put your CV object into the "resamples" argument
		resamples = dat_cv,
		# You don't have to put the parameters object in, but I highly recommend
		# that you get into the habit of always doing it, because sometimes it
		# is required and it can be difficult to tell when.
		param_info = tree_parms,
		# The grid we created goes into the grid argument. You can also put in a 
		# number and the function will automatically make a grid for you
		grid = tree_grid,
		# Our metric set goes here, the tuning method will optimize based on the
		# first metric in the list (RMSE for us)
		metrics = metrics,
		# You can pass other options into here, setting verbose = TRUE will
		# print more information about what is going on.
		control = tune::control_grid(
			verbose = TRUE
		)
	)
```

And now our model is tuned! The `tree_res` object stores the performance
results from all of those hundreds (or thousands depending on how many
parameter combinations you check) of model runs. Now we'll quickly go
through the other two models, only stopping to chat where there are new
details.

## LASSO model

Our LASSO workflow will actually look very similar to our previous linear
regression workflows -- however, we want to use the `glmnet` engine instead
of the `lm` (or `glm`) engine. By looking at the documentation for `linear_reg()`,
you can see that in order to do LASSO we should manually set the argument
`mixture = 1`, while we'll `tune()` the penalty argument, which determines
the "amount of LASSO-ness" (completely technical term) that our model
should have.

```{r}
# Create the recipe
lasso_spec <-
	parsnip::linear_reg(
		mixture = 1,
		penalty = parsnip::tune()
	) |>
	# Decision trees can be for either classification or regression so you
	# should always set the mode for this model!!
	parsnip::set_mode("regression") |>
	# Set the engine -- I recommend that you do this even though we are using
	# the default because the tidyverse/tidymodels teams sometimes like
	# to change the defaults
	parsnip::set_engine("glmnet")

# Put the recipe and model together into a workflow
lasso_wf <-
	workflows::workflow() |>
	workflows::add_recipe(rec_bodytemp) |>
	workflows::add_model(lasso_spec)
```

Now we'll set up a grid and do tuning the same way. This time I'll use the
automatically generated grid.

```{r}
# Extract the parameters from the model
lasso_parms <- hardhat::extract_parameter_set_dials(lasso_spec)

# Build a regularly sampled grid
lasso_grid <- dials::grid_regular(lasso_parms, levels = 10)

lasso_grid
```

And now we tune the model.

```{r}
#| message: false
#| warning: false
#| cache: true
#| cache-lazy: false
lasso_res <-
	lasso_wf |>
	tune::tune_grid(
		resamples = dat_cv,
		param_info = lasso_parms,
		grid = lasso_grid,
		metrics = metrics,
		control = tune::control_grid(
			verbose = TRUE
		)
	)
```

OK, all is going well so far. Note that if you get the `A correlation
computation is required but estimate is constant` warning, you can
pretty much just ignore it. This doesn't matter unless you want to evaluate
on some specific metrics (mainly $R^2$, but we're using RMSE).

Now onto the final and most complex model, the random forest.

## Random forest model

We set the random forest up in the same way as the other models…mostly. Note
that we've specified a few additional arguments in the `set_engine()` part.
These arguments are passed directly to the underlying implementation, which
here is the `ranger` package. The `num.threads` argument allows our model
to be evaluated in parallel, which can often speed up these very complicated
models like random forest. The `importance = "permutation"` argument will
allow us to easily get the variable importance later, a measurement of which
variables are the most predictive of the outcome.

```{r}
rf_spec <-
	parsnip::rand_forest(
		mtry  = parsnip::tune(),
		min_n = parsnip::tune(),
		trees = 1000
	) |>
	parsnip::set_engine(
		"ranger",
		num.threads = 8,
		importance = "permutation"
	) |>
	parsnip::set_mode("regression")

rf_wf <-
	workflows::workflow() |>
	workflows::add_recipe(rec_bodytemp) |>
	workflows::add_model(rf_spec)
```

Now, for the random forest we have an additional step that we need to
do before we can start tuning. For this model, we must **finalize** the
parameter set, because the hyperparameter `mtry` has a *data-dependent range*,
from 1 to the number of predictors in our training data. The easiest
way to do this is like I will show here. 

:::{.callout-note collapse="true"}
Note that I sort of lied a little bit here. For this example, you again don't
*need* the parameter set argument at all, if you use grid tuning and manually
specify the grid argument. You only *need* to finalize this if you want to have
`tidymodels` automatically generate a grid for you, or if you want to use a more
advanced (likely better) tuning method. So I thought I'd include how to do this
anyways since the error you can get stumped both Andreas and me the first time
we encountered it in 2021.
:::

```{r}
rf_parms <-
	rf_spec |>
	hardhat::extract_parameter_set_dials() |>
	dials::finalize(dat_train |> dplyr::select(-BodyTemp))
```

Now we can create our training grid. Note that the random forest model actually
has three tuneable hyperparameters, but I've manually specified the value
for the `trees` parameter -- manually tuning this parameter often has
very little effect on performance. Maybe 500 vs 1000 vs 2000 trees will make a
difference in performance, but spacing the number of trees out on a grid
like 500 vs 570 vs 610 or whatever will not be very impactful.
Now, for `mtry` since we only have a very limited parameter range, I do
think it is worthwhile to try every possible value. That will
require quite a bit of evaluation, so we'll only try a few different
values of `min_n`, although in practice it's probably easier to just do
something like Latin hypercube sampling (or not use grid evaluation). If you want
to talk about the tuning methods I typically like to use, feel free to ask
me in Slack or email.

```{r}
rf_grid <-
	tidyr::expand_grid(
		mtry = seq(1, 24, 1),
		min_n = c(2, 10, 50, 100, 250)
	)

rf_grid
```

Ok, now let's get to tuning. I imagine this one will take a bit of
time.

```{r}
#| message: false
#| warning: false
#| cache: true
#| cache-lazy: false
rf_res <-
	rf_wf |>
	tune::tune_grid(
		resamples = dat_cv,
		param_info = rf_parms,
		grid = rf_grid,
		metrics = metrics,
		control = tune::control_grid(
			verbose = TRUE
		)
	)
```

Ok, great. Now all three of our models are tuned and we can move on to
evaluating them.

# Model evaluation

For each of these models we need to do the following steps:

1. Choose the best set of hyperparameters.
1. Get the RMSE and compare to the null model.
1. Make the diagnostic plots as described in the instructions.

## Decision tree

OK, first let's look at the performance of our different tuning parameters.

```{r}
tree_res |>
	ggplot2::autoplot() +
	zlib::theme_ms()
```

Interestingly, we see that a tree depth of 2 is always the best -- this is
probably because trees with a higher depth tend to overfit much more. By
constraining the tree depth, we get a more transportable model across our
CV folds. Also notice that for a high depth tree (10), a higher cost
complexity is much more important, whereas for a lower depth tree, the cost
complexity does not seem to matter as much. While we probably need more
data points to really evaluate this, that is usually true, because a high
cost complexity forces the tree to have less splits. So if our tree depth is
low, there are not that many splits that are available (a tree of depth 2
has 4 total split options) and the cost complexity is not so important.
Also for low tree depth, the minimal node size is not so important, mostly
for the same reason (there are just not that many splitting points available,
so there is a lot of data at each node).

Okay, we've now evaluated all of those hundreds (or thousands depending on how
many parameter combinations you use) of models and we're ready to move on
to the next step -- *finalizing* our workflow. This basically means we need
to identify the "best" parameter set, and replace the `tune()` placeholders
in our workflow with those values. Fortunately this is pretty easy to do in
tidymodels.

```{r}
# Select best parameter values after optimization
best_tree_sa <- tune::select_best(tree_res, metric = "rmse")

# Finalize workflow with best values
final_tree_wf <-
	tree_wf |>
	tune::finalize_workflow(best_tree_sa)

final_tree_wf
```

In this output, you can see the final selected parameters. Now that we've
selected the best parameters (according to cross-validated RMSE), we want to
see what the actual RMSE was. We can compare the RMSE on the training set,
or the cross-validated RMSE. In 99% of cases, the **cross-validated metrics
are more appropriate.** If you have a specific validation holdout set
(in addition to the test/train sets), you can use the performance on that,
but if you don't you usually want to look at the cross-validated RMSE.

```{r}
tree_mets <-
	tree_res |>
	tune::collect_metrics() |>
	dplyr::filter(.config == best_tree_sa$.config)

tree_mets
```

If you were writing this up for a paper, I would recommend writing a nice
function that formats this how you want it so you can just make a nice table
with one row for each model and columns showing the metrics of interest. But
since I am not being graded, I'll leave that as an exercise for the interested
reader. 

You can see that the (cross-validated) tree RMSE is $`r round(tree_mets[[2, 6]], 2)` \pm `r round(tree_mets[[2, 8]], 2)`$ compared to our null RMSE that we got
earlier of $`r round(null_metrics[[1, 3]], 2)`$. Note that there is not
a lot of difference, the tree model is doing a bit better but really not
that much better. We can also get the predictions on the entire training
set and compare that RMSE. This is usually a good idea (*unless you have a
separate validation set, then you don't need to do this*), but typically produces
a more optimistic estimate than the CV error. However, with the predictions on
the entire dataset we can look at model diagnostics and calibration.

```{r}
# Fit model to training data and augment the predictions to data frame
tree_train_fit <- final_tree_wf |> parsnip::fit(data = dat_train)

# Get the predictions on the training data
dat_train_tree_fit <-
	broom::augment(tree_train_fit, new_data = dat_train) |>
	# calculate the residuals, we will need them later
	dplyr::mutate(.resid = BodyTemp - .pred)

# Get the RMSE and compare to null
tree_metrics <-
	dat_train_tree_fit |>
	metrics(truth = BodyTemp, estimate = .pred)

tree_metrics
```

So in this case, the training data set metrics are about the same as the CV
metrics, which is good. If the training set and CV metrics are way different,
you should probably go back to the drawing board on tuning and resampling
and see if you've done anything wrong. It doesn't always mean you've done
something wrong, but you don't want this estimate to be *wildly* better than the
CV estimate. Now let's look at the diagnostic plots.

```{r}
# Plot Predictions vs observed values
rf_pred_obs_plot <-
	ggplot(dat_train_tree_fit, aes(y = BodyTemp, x = .pred)) +
	geom_abline(slope = 1, intercept = 0, color = "red", linetype = 2) +
	geom_jitter(height = 0, width = 0.01, alpha = 0.5) +
	cowplot::theme_cowplot() +
	labs(
		title = "Decision tree: observed vs fitted",
		y = "Observed",
		x = "Fitted"
	)
# Plot model predictions vs residuals
rf_pred_res_plot <-
	ggplot(dat_train_tree_fit, aes(y = .resid, x = .pred)) +
	geom_hline(yintercept = 0, color = "red", linetype = 2) +
	geom_jitter(height = 0, width = 0.01, alpha = 0.5) +
	cowplot::theme_cowplot() +
	labs(
		title = "Decision tree: residuals vs fitted",
		y = "Residuals",
		x = "Fitted"
	)
# Print both plots together
# I think the "patchwork" package is a better more modern way to do this,
# but I copied and pasted this code from a while back
rf_diag <-
	cowplot::plot_grid(
		rf_pred_obs_plot, rf_pred_res_plot, ncol = 2
	)
rf_diag
```

Well, I think it is safe to say that this model does not look too great based
on the diagnostics. We are only predicting four discrete values (because our
best tree had a depth of 2), so we are obviously missing a lot of variation
in our outcome. You can also notice that a lot of our residuals are high,
indicating that we have a few very high residuals, which is explained by
our model not being able to predict the highest temperature values at all.
So a better model would have a larger range of predicted outcomes and would be
better at fitting the few high outcomes, without sacrificing performance on
the lower outcomes.

## LASSO model

OK, first let's look at the tuning parameters.

```{r}
lasso_res |>
	ggplot2::autoplot() +
	zlib::theme_ms()
```


## Random forest model

```{r}
rf_res |>
	ggplot2::autoplot() +
	zlib::theme_ms()
```


# Comments for specific models

# Model selection

# Final evaluation

<!-- END OF FILE -->
