---
title: "Flu analysis: Basic model fitting"
output:
  html_document:
    toc: FALSE
---

```{r setup, include = FALSE}
box::use(
	tidymodels,
	readr,
	gtsummary,
	purrr
)
```

First we'll load the cleaned data.

```{r}
dat <- readr::read_rds(here::here("fluanalysis", "data", "clean-data.Rds"))
```

Now we will fit some basic regression models to both of our main
outcomes of interest: Gaussian (aka ordinary linear) regression for
body temperature, and logistic (aka logit-link binomial/Bernoulli GLM) for
nausea.

Let's do the models for body temperature first. While I usually don't like
attaching entire packages (especially metapackages) into the global namespace,
for this project I'll go ahead and do that because tidymodels is kind of
irritating if you don't.

```{r}
library(tidymodels)
```

# Body temperature models

First we'll define our "model specification" -- you can think about us setting
up our kitchen equipment. If we were baking cookies, the model specification
would be the part where you drag the stand mixer out of the bottom cabinet
and preheat the oven. We are specifying the "machinery" that we will use to
fit our model.

```{r}
# Gaussian models: BodyTemp is the outcome of interest
linear_mod <- linear_reg() |>
  set_mode("regression") |>
  set_engine("lm")
```

OK, since we're only fitting simple models with no preprocessing steps, we
won't go through all the fuss that we normally would for a robust machine
learning model setup -- we'll do that in the next few modeling steps.
For now I think it's sufficient to take our model specification and just
fit a specific formula. So we'll first fit a model with only `RunnyNose`, and
then we'll fit a model with all of the predictors -- note that in `R`'s formula
syntax, you can use `.` to mean "all variables that I have not already
said in this formula." (See `?formula` for more details.)

```{r}
# Model 1: RunnyNose only
temp_mod_simple <- linear_mod |>
  fit(BodyTemp ~ RunnyNose, data = dat)

# Model 2: all predictors
temp_mod_all <- linear_mod |>
  fit(BodyTemp ~ ., data = dat)
```

Wait, that is it? Yes, indeed, now our two models are fitted. We may want
to manually take a look at them which is quite easy.

```{r}
print(temp_mod_simple)
```

Or we can get more information by *extracting the engine fit*, which will
then allow base R to recognize the object.

```{r}
temp_mod_simple |>
	parsnip::extract_fit_engine() |>
	summary()
```

I know Andreas said no p-values, but it's very difficult to tell this
function not to show them!

This basically works the same for any `glm()` type object and for most things
that come from base `R` (well, from `stats`, not `base`, but it's automatically
loaded for you either way). We can do the same thing for the other model.

```{r}
print(temp_mod_all)
temp_mod_all |>
	parsnip::extract_fit_engine() |>
	summary()
```

Here you can see the "3 not defined because of singularities" message in
the coefficients section. I'll explain this in a bit.

:::{.callout-note}
You can also see an "L", "Q", and "C" after the names of the ordinal predictors.
That is because in `R`, ordinal variables are parametrized as *orthogonal
polynomials* by default: basically the levels are coded as 1, 2, 3, 4 (in the
order none, weak, moderate, severe if you specified the coding correctly) and
this numerical variable is included in the model as a linear effect (L), as a
quadratic effect (Q, aka squared), and as a cubic effect (C, aka cubed). The
orthogonal part just means that these variables are constructed in a way that
the L, Q, and C effect are independent of each other.
:::

OK, so now let's get into the part of comparing the model fits. If you've
taken BIOS 8010 or 8020 in our department, you are probably most
familiar with something like the following.

```{r}
lm_list <-
	list(temp_mod_simple, temp_mod_all) |>
	purrr::map(parsnip::extract_fit_engine)
rlang::exec(anova, !!!lm_list, test = NULL)
```

:::{.callout-tip collapse="true"}
## What the heck is that code

So of course you could do the same thing as above by doing something like

`anova(parsnip::extract_fit_engine(temp_mod_simple), parsnip::extract_fit_engine(temp_mod_all))`

but I suspect that doesn't even fit on the screen correctly. This is just a
way that will allow you to do that for a lot of lm models at one time, if you
need to. If you want to read about the details, you can look at the `rlang`
documentation, but it isn't necessary for this class.
:::

And then based on this ANOVA table, you would either do a statistical test
to determine if the reduction in the sum of squares is significant or
whatever. Here I didn't show the p-value because Andreas and I both think
that you should decide on your own whether that reduction in sum of squares
is meaningful for your problem. However, that can be kind of hard because
we aren't used to think about squared units. So instead look at some
other metrics that are pretty easy to get.

:::{.callout-warning}
## Rank-deficiency

Note that depending on what you tried to do here, you might have gotten a
warning that says something about "rank deficiency". This is caused by the same
issue that gave us 3 coefficients "not defined because of singularities". this
is the same thing. Our cough, myalgia, and weakness yes/no variables do not
provide any information that is different from what's in the severity variables
(once you know someone's severity, you automatically know their response to the
yes/no variable). Our model doesn't know what to do in this case, which is why
we get these warnings! For now you should ignore them but you shouldn't ignore
these warnings in real life and we won't in the next modeling assignment.

(If you know some linear algebra or stat theory,
this is because our cough, myalgia, and weakness yes/no variables are 
linearly dependent on the ordinal versions of those variables and thus
the coefficients for those variables are not estimable.)
:::

So when we look at these models in terms of RMSE, the difference is not that big,
but the $R^2$ is certainly larger, by about 10%. However, you may recall from
your stat classes that this $R^2$ is not **adjusted** and thus will always
increased when we add more predictors. Since we are building predictive models,
that doesn't typically matter as much as it does when we're trying to build
inferential models. But you should certainly notice that the changes in
RMSE and MAE are both much smaller in magnitude, indicating that the
model with more predictors doesn't do that much better of a job at prediction.

We'll use the `broom::glance()` function to get metrics now with a more
in-depth discussion of model evaluation in the next model.

```{r}
temp_simple_res <- broom::glance(temp_mod_simple)
temp_all_res <- broom::glance(temp_mod_all)

temp_simple_res
temp_all_res
```

As you can see, we get the unadjusted and adjusted $R^2$, and it seems
that the model with more predictors does fit a bit better. Although this is
also an interesting case where the AIC and BIC can disagree with each other
on the best model (this happens most often when you are comparing models with a
large difference in the number of parameters).

OK, now let's move on to the logistic regression models, they are basically
the same so you won't get as much commentary from me this time.

# Logistic regression models

First set up the model spec and fit the models.

```{r}
# Logistic models: Nausea is the outcome of interest
logistic_mod <- logistic_reg() |>
  set_mode("classification") |>
  set_engine("glm")

# Model 1: RunnyNose only
nausea_mod_simple <- logistic_mod |>
  fit(Nausea ~ RunnyNose, data = dat)

# Model 2: all predictors
nausea_mod_all <- logistic_mod |>
  fit(Nausea ~ ., data = dat)
```

Now we'll peek at the model summaries.

```{r}
nausea_mod_simple |>
	parsnip::extract_fit_engine() |>
	summary()

nausea_mod_all |>
	parsnip::extract_fit_engine() |>
	summary()
```

We can again look at the analysis of deviance table (the equivalent to
analysis of variance for a non-Gaussian GLM), but again remember that this
is just for comparison.

```{r}
glm_list <-
	list(nausea_mod_simple, nausea_mod_all) |>
	purrr::map(parsnip::extract_fit_engine)
rlang::exec(anova, !!!glm_list, test = NULL)
```

And now let's look at the `glance()` results.

```{r}
nausea_simple_res <- broom::glance(nausea_mod_simple)
nausea_all_res <- broom::glance(nausea_mod_all)

nausea_simple_res
nausea_all_res
```

These results look a bit different from the linear regression results --
here, we can't do the $R^2$ in the same way that we can for the linear
regression models. The best equivalent metric is the **deviance**, which is
penalized by the number of parameters in the AIC/BIC. Again, we'll discuss the
correct way to interpret model results and goodness-of-fit in the next module,
but just know that Andreas and I typically don't like the AIC/BIC or the p-value
approach.

# Save results

Ok, finally we'll save our results. For now I'll save the `glance()` results,
that way we could use them while writing up our reports. For example, you
could include some text like this in your report:

`` `r round(nausea_simple_res$AIC, 2)` ``

and that lets you easily type things like "The AIC for the logistic regression
model with only runny nose status as a predictor was `r round(nausea_simple_res$AIC, 2)`,
while the AIC for the full model with all predictors was `r round(nausea_all_res$AIC, 2)`"
without having to type the numbers -- check the Quarto document for this page
for an example if you want to.

```{r save results}
saveRDS(temp_simple_res, file = here::here("fluanalysis", "results", "temp_simple_res.Rds"))
saveRDS(temp_all_res, file = here::here("fluanalysis", "results", "temp_all_res.Rds"))
saveRDS(nausea_simple_res, file = here::here("fluanalysis", "results", "nausea_simple_res.Rds"))
saveRDS(nausea_all_res, file = here::here("fluanalysis", "results", "nausea_all_res.Rds"))
```

<!-- END OF FILE -->
