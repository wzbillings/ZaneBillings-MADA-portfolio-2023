---
title: "Tidy Tuesday Exercise"
output: 
  html_document:
    toc: FALSE
---

```{r setup}
# I made the setup chunk visible so everyone can see what I did before starting
# my explanations

# Explicit declaration of dependencies
box::use(
	tidytuesdayR,
	ggplot2[...],
	zlib,
	dplyr,
	tidyr,
	colorblindr
)

# Set ggplot theme
ggplot2::theme_set(
	zlib::theme_ms() +
		ggplot2::theme(
			plot.background = ggplot2::element_rect(fill = "white", color = "white"),
			axis.text = ggplot2::element_text(size = 16, color = "black"),
			axis.title = ggplot2::element_text(size = 18),
			plot.subtitle = ggplot2::element_text(
				size = 16, hjust = 0, margin = ggplot2::margin(b = 2)
			),
			plot.title = ggplot2::element_text(
				size = 24, hjust = 0, margin = ggplot2::margin(b = 4)
			),
			plot.caption = ggplot2::element_text(size = 14),
			strip.text = ggplot2::element_text(
				size = 16, hjust = 0.5,
				margin = ggplot2::margin(b = 2, t = 2, l = 2, r =2)
			),
			panel.spacing = ggplot2::unit(2, "lines"),
			legend.position = "bottom",
			legend.text = ggplot2::element_text(size = 16, color = "black"),
			legend.title = ggplot2::element_text(size = 18, color = "black")
		)
)

# Set global options
options(tidymodels.dark = TRUE)
```

This week's **Tidy Tuesday** dataset is on egg production! As an egg enjoyer,
I am very excited about this. Although, I do think it's interesting that the
data only extend through 2021 -- we're currently going through an extremely
severe avian influenza outbreak that has impacted egg production in the US,
so it would be interesting to see how the trends change. (It would also
be interesting to compare the drop in egg production in cage-free vs
non-cage-free chickens if that data is available, as a proxy for whether
there is increased avian influenza burden in non-cage-free chickens, but we
can't do any of that with this data so I'll stop talking about it now.)

# Data loading

But anyway, we first need to start by loading the data. This week there
are two datasets, which is explained on the [documentation page](https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-11).
I originally copied and pasted the code from their site, but it was broken.
(So I submitted a PR and we will see if it gets merged.) But you have
to add the hyphens to the names (and thus the backticks) if you use the
`tidytuesdayR` package to load the data.

```{r}
tuesdata <- tidytuesdayR::tt_load('2023-04-11')

eggproduction <- tuesdata$`egg-production`
cagefreepercentages <- tuesdata$`cage-free-percentages`
```

# Initial exploration and wrangling

OK, first things first let's just look at the data sets.

```{r}
dplyr::glimpse(eggproduction)

dplyr::glimpse(cagefreepercentages)
```

The first thing that I notice is that the `cagefreepercentages` observed months
are not the same as the observed months in the `eggproduction` data, so
we cannot immediately join the data together. We would have to make some
assumptions about how to combine the dates if we wanted to do that.
Otherwise, the `cagefreepercentages` data looks pretty simple, with two
time series observations and the months of observation, and no other
variables.

The `eggproduction` data appears to also have two main time series observations,
`n_hens` and `n_eggs`, along with the month of observation, and two different
covariates, the `prod_type` and the `prod_process`, which are explained in the
data dictionary. There should be 2 unique values of `prod_type` and 3 unique
values of `prod_process` (although one of them is `all` which always kind
of annoys me). Let's take a look at the number of observations in each of
these strata.

```{r}
with(
	eggproduction,
	table(prod_type, prod_process)
)
```

OK, so the data are balanced in terms of the month of observations. We see
that there is no cage-free designation for hatching eggs, which is why we
have the `all` level. So I think we can organize the data a little bit
differently, in a way that will make it a bit easier to explore. I think
we should have a variable with the levels "hatching eggs", "non-organic table
eggs", and "organic table eggs". In order to make sure we can do that,
let's first check that the non-organic table eggs and organic table eggs
add up to make 'all' table eggs.

```{r}
# It will be easier if we check n_hens and n_eggs separately. So first we'll
# do n_eggs
eggproduction |>
	dplyr::filter(prod_type == "table eggs") |>
	dplyr::select(-prod_type, -source, -n_hens) |>
	tidyr::pivot_wider(names_from = prod_process, values_from = n_eggs) |>
	dplyr::mutate(
		sum = `cage-free (non-organic)` + `cage-free (organic)`,
		pct = sum / all * 100
	)
```

OK, from the header of this dataset we can see that these two categories DO NOT
add up the way I thought they should. There's apparently way more in the "all" category than just these two. So I went back
to the data dictionary and saw that it says "The value 'all' includes cage-free and conventional housing" which does not make a lot of sense to me. So I guess the "all" category
also includes non-cage-free eggs. For an actual analysis, we might want to
think about representing the data a bit different, so e.g., we might want a
variable with levels "not cage free", "cage free organic", and "cage free
non-organic" instead of having "all" as a level, but for the purposes of
this analysis I'll leave it as is. I'll come back to this idea of reorganizing
the data in a few cleaning steps.

Now let's try to put the two datasets together.  First I'll need
to remove the "source" columns from each dataset, since those won't be useful
in our analysis anyways, then I'll do a FULL join -- this will keep at least
one copy of each record in both datasets, and will only join together the
records that match. I'll explain my motivation for doing this in a few steps.
I've also specified in the join command that this join should be **many-to-one**,
which means that there are multiple records in `eggproduction` (the left data
set) that can match each record in `cagefreepercentages` (the right
data set), because we want the `cagefreepercentages` to be duplicated for
each of our strata for each month in the `eggproduction` dataset. Finally,
I sorted by the observed month, an I'll explain why in the next step.

```{r, error=TRUE}
joined_data <-
	dplyr::full_join(
		eggproduction |> dplyr::select(-source),
		cagefreepercentages |> dplyr::select(-source),
		"observed_month",
		relationship = "many-to-one"
	) |>
	dplyr::arrange(observed_month)
```

OK, so that did not work. You can see we got an error, and this is because of
the "many-to-one" relationship that I specified. We actually got a **many-to-many**
merge when dplyr compared the key sets of the two data frames. That means that
there are some months with multiple records in the `cagefreepercentages`
dataset, so let's take a look at that.

```{r}
count_months <-
	cagefreepercentages |>
	dplyr::count(observed_month) |>
	dplyr::filter(n > 1)

count_months
```

There are five observed months with two observations! Let's take a look at
those ones.

```{r}
cagefreepercentages |>
	dplyr::filter(observed_month %in% count_months$observed_month)
```

OK, interestingly we have some records with multiple sources. I guess I
shouldn't have just dropped the source without looking at it! For this
exploration, I'm going to *drop the rows with computed as the source.* That might
not be the best choice to make here, but it will make my life easier so that's
what I'm going to do. In a real life setting we would want to think about this
a little harder.

For the `eggproduction` data, we know that each stratum was measured the same
number of times, so I don't think we have the same issue there. But we'll see
if anything fishy shows up later. So now let's go ahead and join the data
then sort by month.

```{r}
cage_free_for_merge <-
	cagefreepercentages |>
	dplyr::filter(source != "computed") |>
	dplyr::select(-source)

joined_data <-
	dplyr::full_join(
		eggproduction |> dplyr::select(-source),
		cage_free_for_merge,
		"observed_month",
		relationship = "many-to-one"
	) |>
	dplyr::arrange(observed_month)

dplyr::glimpse(joined_data)
```

OK, so you can see here that our data now starts with a bunch of NAs!
That's because there are some dates in the `cagefreepercentages` data that
are earlier than anything in `eggproduction`, and when we do a full join,
records that don't have a match get the columns from the other data filled in
with missing values.

So here's why I wanted to join the data together like this: we are going to
make the assumption that if we measured `percent_hens` and `percent_eggs`
at some time, *that value doesn't change until the next measurement*. Yes, this
is a completely unrealistic assumption, but it is often a useful assumption,
because then we can fill in the "gaps" for the measurements that were made
at different times in the two datasets, so we get more useful data points.
To do this, we can just "fill down" the values in the `percent_hens` and
`percent_eggs` column -- one we get a measured value, go down the column
and replace NAs with that measurement, until we get to the next measured value.

```{r}
filled_data <-
	joined_data |>
	tidyr::fill(percent_hens, percent_eggs, .direction = "down")
```

Now, since I think our main question of interest will be about the data from
`eggproduction`, let's filter out all the rows with dates that did NOT match
that dataset.

```{r}
filtered_data <-
	filled_data |>
	dplyr::filter(observed_month %in% eggproduction$observed_month)

dplyr::glimpse(filtered_data)
```

OK, now it looks like there is only one more thing to check to me -- the
`percent_eggs`. It looks like it could be all NAs.

```{r}
sum(is.na(filtered_data$percent_eggs)) / nrow(filtered_data)
```

Yep, it is 100% NAs. I bet all the values from that column were from the
`computed` source that I filtered out. If one wanted to, we could fill
in those values the same way, or deal with this in a few other ways,
but for now I am just going to drop that column. But looking at the egg
production might actually be interesting, so one should really consider
if dropping it is the right call.

```{r}
filtered_data2 <-
	filtered_data |>
	dplyr::select(-percent_eggs)
```

Now that I've been looking at it, I still think we can reclassify the
`prod_type` and `prod_process` variables into one variable to make our
lives a bit easier, so that will be my last cleaning step.

```{r}
model_data <-
	filtered_data2 |>
	dplyr::mutate(
		egg_type = ifelse(
			prod_type == "hatching eggs", "hatching eggs", prod_process
		),
		egg_type = factor(
			egg_type,
			levels = c("hatching eggs", "all", "cage-free (organic)",
								 "cage-free (non-organic)"),
			labels = c(
				"hatching eggs",
				"table eggs (other)",
				"table eggs (cage-free organic)",
				"table eggs (cage-free non-organic)"
			)
		)
	) |>
	dplyr::select(
		observed_month,
		egg_type,
		n_hens,
		n_eggs,
		percent_hens
	)

dplyr::glimpse(model_data)
```

OK, so now I think our data is ready to go. Right now I am thinking that
**predicting the number of eggs** based on time, number of hens, and percent
of cage free eggs, stratified by egg type, might
be a neat question to ask. So let's do some more EDA in that direction.

# EDA

First let's make a time-series plot of the different variables so we can
see how all of them change over time. First I'll make a plot for the
number of eggs, the outcome.

```{r}
model_data |>
	ggplot() +
	aes(
		x = observed_month,
		y = n_eggs,
		color = egg_type
	) +
	geom_line(linewidth = 2) +
	scale_y_continuous(
		labels = scales::label_number(scale_cut = scales::cut_short_scale()),
		trans = "log10"
	) +
	colorblindr::scale_color_OkabeIto() +
	labs(
		y = "Number of eggs (log scale)",
		x = "Month",
		color = NULL
	) +
	guides(color = guide_legend(ncol=2))
```

It looks like there's some variation in the trends over time that we can model,
although it is clear that non-organic cage-free table eggs definitely has the
strongest trend. Now let's look at the number of hens.

```{r}
model_data |>
	ggplot() +
	aes(
		x = observed_month,
		y = n_hens,
		color = egg_type
	) +
	geom_line(linewidth = 2) +
	scale_y_continuous(
		labels = scales::label_number(scale_cut = scales::cut_short_scale()),
		trans = "log10"
	) +
	colorblindr::scale_color_OkabeIto() +
	labs(
		y = "Number of hens (log scale)",
		x = "Month",
		color = NULL
	) +
	guides(color = guide_legend(ncol=2))
```

There appears to be a lot less variation, in general, in the number of hens,
although we can see the same macro-level trends over time. I don't know that
there's really anything else that's all that interesting to say here. Now
let's look at the percent of cage-free hens trend over time.

```{r}
model_data |>
	dplyr::select(observed_month, percent_hens) |>
	dplyr::distinct() |>
	ggplot() +
	aes(
		x = observed_month,
		y = percent_hens
	) +
	geom_line(linewidth = 2) +
	scale_y_continuous(
		labels = scales::label_percent(scale = 1)
	) +
	labs(
		x = "Month",
		y = "Percentage of hens that are cage-free"
	)
```

We can see a pretty strong increasing trend here, with only a few dips. It will
be interesting to see how well this trend correlates with the others! Speaking
of correlations, I think we are kind of limited with the types of plots we
can make here, but I think we can directly look at those kind of correlations.

```{r}
model_data |>
	ggplot() +
	aes(
		x = n_hens,
		y = n_eggs
	) +
	geom_point() +
	geom_smooth(
		formula = 'y ~ x',
		method = "lm"
	) +
	facet_wrap(vars(egg_type), scales = "free") +
	scale_x_continuous(
		labels = scales::label_number(scale_cut = scales::cut_short_scale()),
		trans = "log10"
	) +
	scale_y_continuous(
		labels = scales::label_number(scale_cut = scales::cut_short_scale()),
		trans = "log10"
	) +
	labs(
		x = "Number of hens",
		y = "Number of eggs"
	)
```

As we might expect, the number of hens and the number of eggs appear to be
highly correlated, with only a few outliers from a strong positive trend. Let's
check the correlation with the percentage of cage-free hens.

```{r}
model_data |>
	ggplot() +
	aes(
		x = percent_hens,
		y = n_eggs
	) +
	geom_point() +
	geom_smooth(
		formula = 'y ~ x',
		method = "lm"
	) +
	facet_wrap(vars(egg_type), scales = "free") +
	scale_y_continuous(
		labels = scales::label_number(scale_cut = scales::cut_short_scale()),
		trans = "log10"
	) +
	scale_x_continuous(
		labels = scales::label_percent(scale = 1)
	) +
	labs(
		x = "Percent cage-free hens",
		y = "Number of eggs"
	)
```

Here, we see a somewhat strong linear relationship, although especially for
table eggs it is clear that the relationship is not the strong. The other
egg types also show much more variation that doesn't appear completely
random. Overall, I expect time to be the strongest predictor of number of eggs,
number of hens, and percentage of cage-free hens. And it is clear that the trends
differ by the egg type. But now I think we can fit some models to try and do
prediction.

# Model fitting

For this analysis, there are 4 candidate models I am interested in trying.

1. Linear regression, everyone's favorite and the simplest model (using
ordinary least squares).
1. Random forest, a flexible alternative to parametric regression.
1. Auto-ARIMA, an easy and flexible time series model.
1. PROPHET, another easy and flexble time series model that sometimes beats
out ARIMA.

And of course we will compare these models to the null model, which will
always predict the mean.

## Data splitting

In order to see how our predictions might perform on new data, we'll
first create a holdout set of data that we won't look at until we've chosen
a model. Since we have time-series data, it is typically better to train
the data on some time period of initial data, and test on a later portion,
rather than choosing random points for our test set. Fortunately, that is
easy using `rsample`.

```{r}
# We actually don't need to set the seed for time-series splitting cause it
# is deterministic, but let's do it for fun and just in case.
set.seed(100)

dat_split <- rsample::initial_time_split(model_data, prop = 3/4)
dat_train <- rsample::training(dat_split)
dat_test  <- rsample::testing(dat_split)
```

Now some of our models also require the selection of hyperparameters, so to
improve our hyperparameter choice, we need to create some resamples that
we can fit our model to. Again, this is more complicated for time series
data than it is for regular data, and it's not something that I really
know a lot about. So I looked at the [rsample website](https://rsample.tidymodels.org/articles/Common_Patterns.html#time-based-resampling)
and I'll use the `rolling_origin()` method for creating resamples because it
seems pretty reasonable in my naive opinion. Since I don't know much
about this, I'll use the default settings.

```{r}
dat_resamples <-
	dat_train |>
	rsample::rolling_origin()
```

## Workflow setup

For tidymodels, we need a `recipe` and one or more model specs, so first I'll
set up the recipe. We've already done most of the preprocessing though, so
the only step we need is to ensure that our fitting takes the egg type
groups into account.

For the types of models I want to use, we'll actually need two recipes -- 
one that leaves the date as a date variable for the `modeltime` models, and one that transforms the
date into a numeric variable that the `tidymodels` models can understand.

```{r}
tm_recipe <-
	recipes::recipe(
		n_eggs ~ date_numeric + n_hens + percent_hens + egg_type,
		data = dat_train
	) |>
	# Make the date into a numeric variable
	recipes::step_mutate(
		date_numeric = as.numeric(date_numeric)
	) |>
	# Change the role of egg_type so it isn't included as a first-order
	# predictor
	recipes::update_role(egg_type, new_role = "grouping") |>
	# Create interaction terms of all first-order predictors with egg_type
	# This is the same as stratifying
	recipes::step_interact(terms = ~recipes::all_predictors() * egg_type)

# Because of some fitting detail about extraneous regressors or something
# we cannot include the interaction terms in the time series models
mt_recipe <-
	recipes::recipe(
		n_eggs ~ observed_month + n_hens + percent_hens + egg_type,
		data = dat_train
	) |>
	# Change the role of egg_type so it isn't included as a first-order
	# predictor
	recipes::update_role(egg_type, new_role = "grouping")
```

Now we need to declare model specifications for each model we want to fit.
I intend to use a `workflowset` in the next step, so I'm going to create
these in a list.

```{r}
model_specs <-
	list(
		"null_model" =
			parsnip::null_model() |>
			parsnip::set_engine("parsnip"),
		"lin_reg" =
			parsnip::linear_reg() |>
			parsnip::set_engine("lm"),
		"rf" =
			parsnip::rand_forest(
				mtry = parsnip::tune(),
				trees = 2000,
				min_n = parsnip::tune()
			),
		"ARIMA" =
			modeltime::arima_reg() |>
			parsnip::set_engine("auto_arima"),
		"PROPHET" =
			modeltime::prophet_boost(
				prior_scale_changepoints = parsnip::tune(),
				seasonality_yearly = FALSE,
				seasonality_weekly = FALSE,
				seasonality_daily = FALSE,
			) |>
			parsnip::set_engine("prophet_xgboost")
	)

# Set the mode to regression for all model specs
model_specs <- lapply(model_specs, \(x) parsnip::set_mode(x, "regression"))
```

Now we'll cross our recipe with our model specs to get a `workflowset`, a
collection of workflows that we can map all of our fitting functions to.

```{r}
preproc_list <-
	c(rep(list(tm_recipe), times = 3), rep(list(mt_recipe), times = 2)) |>
	rlang::set_names(c(rep("tm", times = 3), rep("mt", times = 2)))


wfs <-
	workflowsets::workflow_set(
		preproc = preproc_list,
		models = model_specs,
		cross = FALSE
	)
```

## Hyperparameter tuning

Now that we have our workflowset ready, we can tune our models. First, I think
this will be pretty slow if we do it all sequentially, so we want to intialize
a parallel processing backend.

```{r}
library(tidymodels)
library(finetune)
library(modeltime)
cl <- parallel::makePSOCKcluster(16)
doParallel::registerDoParallel(cl)
```

Now we can make a tuning routine to all of our workflows. Normally I mess
around with this a bit more, but I'm feeling lazy today, so I'll just
copy and paste (and change variable names) the code I used for my most
recent machine learning project. I like this one because it automatically
finalizes the data-dependent parameters, and it's pretty fast. If I really wanted
to make sure something was tuned well, I would use `tune_bayes()` or
`tune_sim_anneal()` but I don't think either of those will automatically
finalize parameters.

```{r}
wfs_res <-
	wfs |>
			workflow_map(
			"tune_race_anova",
			resamples = dat_resamples,
			grid = 25,
			metrics = yardstick::metric_set(mae, rmse, rsq),
			verbose = TRUE,
			seed = 370,
			control = control_race(
				verbose = TRUE,
				event_level = "second",
				allow_par = TRUE,
				parallel_over = "everything"
			)
		)
```

```{r}
gam_spec <-
	parsnip::gen_additive_mod(
		select_features = FALSE,
		adjust_deg_free = FALSE
	) |>
	parsnip::set_engine("mgcv") |>
	parsnip::set_mode("regression")

test_wf <- workflow(tm_recipe) |> add_model(gam_spec, formula = gam_fmla)

test <-
	fit_resamples(test_wf, resamples = dat_resamples, metrics = metrics)
```

```{r}
spec <-
	modeltime::arima_reg(
		seasonal_period = 1,
		seasonal_ar = 0,
		seasonal_differences = 0,
		seasonal_ma = 0,
		non_seasonal_ar = 10,
		non_seasonal_differences = 10,
		non_seasonal_ma = 10
	) |>
	parsnip::set_engine("auto_arima") |>
	parsnip::set_mode("regression")

test_wf <-
	workflows::workflow() |>
	add_recipe(mt_recipe) |>
	add_model(spec)

test <-
	fit(
		spec,
		n_eggs ~ observed_month + egg_type,
		data = dat_train
		#resamples = dat_resamples,
	#	grid = 25,
	#	metrics = yardstick::metric_set(mae, rmse, rsq),
		# seed = 370,
		# control = control_resamples(
		# 	verbose = TRUE,
		# 	allow_par = TRUE,
		# 	parallel_over = "everything"
		# )
	)

```




<!-- END OF FILE -->
