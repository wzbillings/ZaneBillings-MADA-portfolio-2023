[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my website.\nHello, everyone! My name is Zane Billings, and I‚Äôm the TA for the MADA course for the Spring 2023 semester. I‚Äôm also a third-year PhD Student in the data analysis and modeling program working with Andreas. My work is about flu immunology and the seasonal flu shot.\n\n\n\nA picture of me, in my office, working on this website!\n\n\n\n\nMy backstory\nI have undergrad degrees in math and biology from the best school ever, Western Carolina University (GO CATS! üòª). I already took this class ‚Äî if you look hard enough you can probably find all my stuff from then! I‚Äôve also taken a lot of other statistics classes (and a few programming classes too), and I use R almost every day. So hopefully I‚Äôll be able to help with any problems that y‚Äôall have. I am hoping to improve my skills on debugging problems and communicating with students in this course üòÅ.\nIn my spare time I like to read (mainly fantasy and horror, but I read all kinds of different stuff) and game (I‚Äôm currently really into Oxygen Not Included). The fun fact I always share is that I have a certificate from the state of Montana that says I can distinguish between black bears and grizzly bears. I‚Äôve never been to Montana, I got the certificate online.\n\n\nLink\n\n\n\nHere‚Äôs one of my favorite data-analysis-related comics.\n\n\nI spend a lot of time trying to tell people that no amount of statistics can fix messed-up data! Truly it is one of the plights of the modern statistician. If you want to go to the comic source, you can go here."
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise."
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "Flu analysis EDA",
    "section": "",
    "text": "The main outcomes we will consider for this analysis are body temperature (continuous) and nausea (categorical). Since there are 30 other potential predictors, we need to decide which predictors to explore. Instead of doing any sort of data-driven method for this, I will just pick some:\n\ncough (ordered);\ncough (prescence/absence);\nchills or sweats (presence/absence); and\nsubjective fever (presence/absence).\n\n\ndat <- readr::read_rds(here::here(\"fluanalysis\", \"data\", \"clean-data.Rds\"))\n\ndat_eda <-\n    dat |>\n    dplyr::select(\n        #Outcomes\n        BodyTemp, Nausea,\n        # Predictors\n        Cough, CoughYN2, ChillsSweats, SubjectiveFever\n    )\n\nThere is no real reason why I chose these ones, but I think comparing subjective fever to body temperature will be interesting. (We actually did this as an exploratory analysis for a paper we recently submitted on these data, and it was interesting how subjective fever didn‚Äôt necessarily seem to line up with body temperature.)\n\nUnivariate distributions\nFirst we will look at the distribution of the only continuous variable, body temperature.\n\n# Define a few functions so that making the summary stats I want will be easier\n# The first function returns the quantiles as a tibble\nget_quantiles <- function(.x, p = c(0, 0.25, 0.5, 0.75, 1)) {\n    q <- quantile(.x, probs = p)\n    out <-\n        tibble::tibble(x = q, probs = p * 100) |>\n        tidyr::pivot_wider(names_from = probs, values_from = x,\n                                             names_glue = \"p{probs}\")\n    return(out)\n}\n# The second function returns the mean and bootstrap CI (b = 1000, alpha = .05)\n# as a tibble\nget_boot_ci <- function(.x, d = 2) {\n    out <-\n        ggplot2::mean_cl_boot(.x) |>\n        dplyr::transmute(\n            mean = round(y, d),\n            \"95% CI\" = paste0(\"(\", round(ymin, d), \",\", round(ymax, d), \")\")\n        )\n    return(out)\n}\n\n# Now generate the summary stats and print as an HTML table\ndat_eda |>\n    dplyr::summarize(\n        get_boot_ci(BodyTemp),\n        \"std. dev\" = round(sd(BodyTemp), 2),\n        \"std. err\" = round(sd(BodyTemp) / sqrt(dplyr::n()), 2),\n        get_quantiles(BodyTemp)\n    ) |>\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\n95% CI\nstd. dev\nstd. err\np0\np25\np50\np75\np100\n\n\n\n\n98.94\n(98.85,99.02)\n1.2\n0.04\n97.2\n98.2\n98.5\n99.3\n103.1\n\n\n\n\n\nFrom the summary statistics, we can see that while the median is fairly close to the mean (within one standard deviation), the third quartile (75th percentile) and the maximum both lie outside of the 95% (bootstrap) confidence interval. This suggests that while the bulk of the data are close together, there are some data points which are much larger than the majority. These could be outliers, or the data could be skewed to the right. We could calculate more summary statistics to try and figure this out, but I think we will have more luck making a plot.\nFirst I‚Äôll set the ggplot2 theme for all subsequent plots.\n\nggplot2::theme_set(\n    zlib::theme_ms() +\n        # Something I need to fix in zlib -- the strip text bottoms get cut off\n        ggplot2::theme(\n            strip.text = ggplot2::element_text(margin = ggplot2::margin(b = 2))\n        )\n)\n\nNow we‚Äôll make a histogram of body temperature observations.\n\nggplot(dat_eda, aes(x = BodyTemp)) +\n  geom_histogram(\n    binwidth = 0.1,\n    color = \"black\",\n    fill = \"gray\",\n    boundary = 0.5\n  ) +\n  scale_y_continuous(expand = expansion(add = c(0, 1))) +\n  scale_x_continuous(name = \"body temperature (degrees F)\")\n\n\n\n\nFrom the histogram, we can see that the data are mostly concentrated around the range given by the CI in the previous table. However, there are a few outliers which are above this, so the data is skewed, but there are not enough large temperature values to dramatically skew the mean of the data. We could consider transforming the data to correct for the skew (e.g.¬†the Box-Cox transformation may help), but this makes the interpretation more annoying so I will not do that.\nNext let‚Äôs look at the distribution of the binary variables.\n\ndat_eda |>\n  dplyr::select(-BodyTemp, -Cough) |>\n  tidyr::pivot_longer(\n    cols = everything()\n  ) |>\n  ggplot(aes(x = value)) +\n  geom_hline(yintercept = nrow(dat_eda) / 2, lty = 2) +\n  geom_bar(color = \"black\", fill = \"gray\") +\n  facet_wrap(vars(name)) +\n  scale_x_discrete(name = NULL) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.01))\n  )\n\n\n\n\nWe can also create this information in table form. For the table I would prefer to have the percentages instead of the counts, but I was too lazy to do this in the plot above.\n\ndat_eda |>\n  dplyr::select(-BodyTemp, -Cough) |>\n  tidyr::pivot_longer(cols = everything(), names_to = \"var\") |>\n  dplyr::group_by(var) |>\n  dplyr::summarize(\n    \"# positive\" = sum(value == \"Yes\"),\n    p = round(mean(value == \"Yes\"), 2),\n    \"95% CI\" = paste0(\n      \"(\", round(p - 1.96 * sqrt((p * (1-p)) / 730), 2), \",\",\n      round(p + 1.96 * sqrt((p * (1-p)) / 730), 2), \")\")\n    ) |>\n  knitr::kable()\n\n\n\n\nvar\n# positive\np\n95% CI\n\n\n\n\nChillsSweats\n600\n0.82\n(0.79,0.85)\n\n\nCoughYN2\n683\n0.94\n(0.92,0.96)\n\n\nNausea\n255\n0.35\n(0.32,0.38)\n\n\nSubjectiveFever\n500\n0.68\n(0.65,0.71)\n\n\n\n\n\nWe can see that the majority of respondents reported chills and sweats and cough. A bit less than half reported nausea and a bit more than half reported a subjective fever.\nFinally let‚Äôs make a plot showing the distribution of our ordered outcome that I picked, the cough severity. Here I‚Äôll put the labels on the plot just the class has an example they can follow for this.\n\ndat_eda |>\n    dplyr::count(Cough) |>\n    dplyr::mutate(\n        perc = n / sum(n),\n        perc_lab = paste0(sprintf(\"%.1f\", perc * 100), \"%\"),\n        text_lab = paste0(n, \" (\", perc_lab, \")\")\n    ) |>\n    ggplot() +\n    ggplot2::aes(x = Cough, y = n) +\n    geom_col(color = \"black\", fill = \"gray\") +\n    geom_text(aes(label = text_lab), size = 6, nudge_y = 15) +\n    labs(\n        x = \"Cough severity\",\n        y = \"Count\"\n    )\n\n\n\n\nI think the most important conclusion from this plot is that almost half of the respondents marked ‚ÄúModerate‚Äù, and very few of them marked ‚ÄúNone‚Äù. Using the ‚ÄúNone‚Äù category for prediction will likely be difficult unless that category is strongly associated with a specific outcome.\n\n\nBivariate associations\nNow we can examine the relationships between our predictors and our outcomes. First we will consider body temperature as the main outcome.\nSince we have a continuous outcome and categorical predictors, let‚Äôs do one of those fancy raincloud plots that people online seem to love these days. Code shamelessly copied from this great tutorial, I was even feeling too lazy to mess with it more like I‚Äôm usually inclined to do.\n\ndat_eda |>\n    tidyr::pivot_longer(\n        cols = -c(BodyTemp, Cough)\n    ) |>\n    ggplot(aes(x = value, y = BodyTemp)) +\n    ggdist::stat_halfeye(\n        adjust = .5, \n        width = .6, \n        .width = 0, \n        justification = -.2, \n        point_colour = NA\n    ) + \n    geom_boxplot(\n        width = .15, \n        outlier.shape = NA\n    ) +\n    ## add justified jitter from the {gghalves} package\n    gghalves::geom_half_point(\n        ## draw jitter on the left\n        side = \"l\", \n        ## control range of jitter\n        range_scale = .4, \n        ## add some transparency\n        alpha = .3\n    ) +\n    stat_summary(\n        geom = \"point\",\n        fun = mean,\n        shape = 9,\n        color = \"red\",\n        size = 2\n    ) +\n    facet_wrap(~name)\n\n\n\n\nI could probably spend some more time fiddling with this plot to make it look nicer but I am not going to, I think it is about 80% to where I want it to be. Anyways, this plot shows the data points, the boxplot, the violin plot density estimate, and the diamond crosshair marks the mean.\nThere does not appear to be a strong relationship between symptoms and any of the body temps, which is possibly distorted by the fact that high body temperatures are rarer in our sample. There is maybe a slight association with ChillsSweats, CoughYN2, and SubjectiveFever, but probably no relationship with nausea.\nAnd then we‚Äôll do it separately for the ordinal outcome just because my life is easier this way (you can‚Äôt pivot an unordered and ordered factor into the same column!).\n\ndat_eda |>\n    ggplot2::ggplot() +\n    aes(x = Cough, y = BodyTemp) +\n  ggdist::stat_halfeye(\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.2, \n    point_colour = NA\n  ) + \n  geom_boxplot(\n    width = .15, \n    outlier.shape = NA\n  ) +\n  ## add justified jitter from the {gghalves} package\n  gghalves::geom_half_point(\n    ## draw jitter on the left\n    side = \"l\", \n    ## control range of jitter\n    range_scale = .4, \n    ## add some transparency\n    alpha = .3,\n    transformation = position_jitter(height = 0)\n  ) +\n    stat_summary(\n        geom = \"point\",\n        fun = mean,\n        shape = 9,\n        color = \"red\",\n        size = 2\n    ) +\n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\n    labs(\n        x = \"Cough severity\",\n        y = \"Body temperature (degrees F)\"\n    )\n\n\n\n\nIt looks like people who reported no cough were more likely to have a low body temperature, but there are a few outliers (note that high temperature plus no cough is an important diagnostic criterion for strep throat, which was likely a common competing risk for influenza in the source study).\nNow we can visualize the relationship between the different variables and the presence of absence of nausea. For this, I will make a separate plot for the categorical predictors and the numerical predictor. First let‚Äôs look at the relationship between nausea and body temperature.\nHere‚Äôs kind of a cool way to visualized what a univariate logistic regression would look like. If you want to see another example, you can see where I copied the code from (tweaking required a bit this time) here. For this plot, presence of nausea is represented by ‚Äú1‚Äù and absence of nausea is represented by ‚Äú0‚Äù. The size of the circle represents the weight of that point (the number of patients which had the same body temperature and nausea status). This allows us to fit a GLM that predicts the probability a patient has nausea based on their body temperature. As you can see from the plot, body temperature has very little predictive power for nausea status.\n\ndat_eda |>\n    dplyr::transmute(\n        BodyTemp,\n        Nausea = ifelse(Nausea == \"Yes\", 1, 0)\n    ) |>\n    ggplot(aes(x = BodyTemp, y = Nausea)) +\n    ggdist::stat_dots(\n        aes(y = Nausea, side = ifelse(Nausea == 0, \"top\", \"bottom\")),\n        scale = 1/3\n    ) +\n    geom_smooth(\n        formula = \"y ~ x\", method = \"glm\",\n        method.args = list(family = \"binomial\")\n    ) +\n  scale_x_continuous(name = \"body temperature (degrees F)\") +\n  scale_y_continuous(name = \"probability of nausea\")\n\n\n\n\nThe logistic regression line is pretty flat, and it‚Äôs hard to compare the distribution of points, so let‚Äôs also make overlapping histograms.\n\ndat_eda |>\n    ggplot(aes(x = BodyTemp, fill = Nausea)) +\n    geom_histogram(\n        aes(y = after_stat(density)),\n        boundary = 0.1,\n        binwidth = 0.1,\n        position = position_identity(),\n        alpha = 0.67,\n        color = \"black\"\n    ) +\n    # These two colors make a colorblind friendly palette\n    scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n    scale_x_continuous(name = \"body temperature (degrees F)\")\n\n\n\n\nIf you think this is two confusing, since BodyTemp is mostly continuous you could also do a density curve, which works well for ‚Äúdensely-measured‚Äù data. So sometimes it will look really bad, and then you should use a histogram instead. But for this it will be ok, and easier to look at.\n\ndat_eda |>\n    ggplot(aes(x = BodyTemp, color = Nausea, fill = Nausea)) +\n    geom_density(\n        kernel = \"epanechnikov\",\n        position = position_identity(),\n        alpha = 0.5,\n        linewidth = 2\n    ) +\n    # These two colors make a colorblind friendly palette\n    scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n    scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n    scale_x_continuous(name = \"body temperature (degrees F)\")\n\n\n\n\nNo matter which of these three plots you prefer, we can see generally the same thing: there is not a clear pattern in the relationship between body temperature and having nausea.\nNow we can see which of the categorical variables are predictive for nausea status. The only real visualization option we have here is a bar chart, but we can choose if we want it to be stacked or clustered.\n\ndat_eda |>\n  dplyr::select(-BodyTemp, -Cough) |>\n  tidyr::pivot_longer(cols = -Nausea) |>\n  ggplot(aes(x = value, fill = Nausea)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(vars(name)) +\n  scale_x_discrete(name = NULL) +\n  scale_y_continuous(labels = scales::percent_format(), name = NULL) +\n    # These colors are also colorblind freindly, but not as good as the previous\n    # ones. Go Cats!!\n  scale_fill_manual(values = c(\"#592C88\", \"#C1A875\")) +\n  theme(legend.position = \"bottom\", legend.justification = \"center\")\n\n\n\n\nLet‚Äôs try the clustered option for our ordinal Cough severity variable.\n\ndat_eda |>\n  dplyr::select(Nausea, Cough) |>\n  ggplot(aes(x = Cough, fill = Nausea)) +\n  geom_bar(position = \"dodge\") +\n  scale_x_discrete(name = \"Cough severity\") +\n  scale_y_continuous(name = \"Count\") +\n  scale_fill_manual(values = c(\"#592C88\", \"#C1A875\"))\n\n\n\n\nNote that you can do things that are a bit fancier, like this option of computing sample proportions with CIs. I typically prefer the simpler plots for exploration though, I just wanted to include as many examples as I could think of in here.\n\ndat_eda |>\n    dplyr::select(Nausea, Cough) |>\n    # Turn Nausea into 0/1 indicator for math stuff\n    dplyr::mutate(Nausea = as.numeric(Nausea == \"Yes\")) |>\n    dplyr::group_by(Cough) |>\n    dplyr::reframe(ggplot2::mean_cl_boot(Nausea)) |>\n    ggplot(aes(x = Cough, y = y, ymin = ymin, ymax = ymax)) +\n    geom_line(aes(group = 1)) +\n    geom_pointrange() +\n    scale_y_continuous(\n        limits = c(0.2, 0.6),\n        breaks = seq(0, 1, 0.1),\n        name = \"Proportion with nausea (95% bootstrap CI)\",\n        labels = scales::label_percent()\n    ) +\n    scale_x_discrete(name = \"Cough severity\")\n\n\n\n\nYou could do something even more complicated and do two lines in different colors that show the counts. But I won‚Äôt do that because I think you probably shouldn‚Äôt do that. I think this last one is already quite complicated and in general you want your audience to not have to think about your charts, or at least exert as little mental effort as necessary. However, from this plot of the proportions, we can see that there appears to be no trend in the proportion of people in our sample with nausea as cough severity increases.\nBased on our charts, there aren‚Äôt any extremely strong predictors of nausea from the set of predictors that I chose, but it looks like cough is not really predictive at all. Out of these, maybe myalgia and weakness are the strongest predictors, but even those are only moderately predictive. However, it may be the case that these symptoms actually interact, so these bivariate associations may not capture all available information.\nOK, that‚Äôs about all for this script. Now we can move on to actual modeling."
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "Flu analysis: Basic model fitting",
    "section": "",
    "text": "First we‚Äôll load the cleaned data.\n\ndat <- readr::read_rds(here::here(\"fluanalysis\", \"data\", \"clean-data.Rds\"))\n\nNow we will fit some basic regression models to both of our main outcomes of interest: Gaussian (aka ordinary linear) regression for body temperature, and logistic (aka logit-link binomial/Bernoulli GLM) for nausea.\nLet‚Äôs do the models for body temperature first. While I usually don‚Äôt like attaching entire packages (especially metapackages) into the global namespace, for this project I‚Äôll go ahead and do that because tidymodels is kind of irritating if you don‚Äôt.\n\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.0.0 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.4     ‚úî recipes      1.0.5\n‚úî dials        1.2.0     ‚úî rsample      1.1.1\n‚úî dplyr        1.1.1     ‚úî tibble       3.2.1\n‚úî ggplot2      3.4.2     ‚úî tidyr        1.3.0\n‚úî infer        1.0.4     ‚úî tune         1.1.0\n‚úî modeldata    1.1.0     ‚úî workflows    1.1.3\n‚úî parsnip      1.0.4     ‚úî workflowsets 1.0.0\n‚úî purrr        1.0.1     ‚úî yardstick    1.1.0\n\n\nWarning: package 'broom' was built under R version 4.2.3\n\n\nWarning: package 'modeldata' was built under R version 4.2.3\n\n\nWarning: package 'parsnip' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'workflows' was built under R version 4.2.3\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ purrr::discard() masks scales::discard()\n‚úñ dplyr::filter()  masks stats::filter()\n‚úñ dplyr::lag()     masks stats::lag()\n‚úñ recipes::step()  masks stats::step()\n‚Ä¢ Use tidymodels_prefer() to resolve common conflicts.\n\n\n\nBody temperature models\nFirst we‚Äôll define our ‚Äúmodel specification‚Äù ‚Äì you can think about us setting up our kitchen equipment. If we were baking cookies, the model specification would be the part where you drag the stand mixer out of the bottom cabinet and preheat the oven. We are specifying the ‚Äúmachinery‚Äù that we will use to fit our model.\n\n# Gaussian models: BodyTemp is the outcome of interest\nlinear_mod <- linear_reg() |>\n  set_mode(\"regression\") |>\n  set_engine(\"lm\")\n\nOK, since we‚Äôre only fitting simple models with no preprocessing steps, we won‚Äôt go through all the fuss that we normally would for a robust machine learning model setup ‚Äì we‚Äôll do that in the next few modeling steps. For now I think it‚Äôs sufficient to take our model specification and just fit a specific formula. So we‚Äôll first fit a model with only RunnyNose, and then we‚Äôll fit a model with all of the predictors ‚Äì note that in R‚Äôs formula syntax, you can use . to mean ‚Äúall variables that I have not already said in this formula.‚Äù (See ?formula for more details.)\n\n# Model 1: RunnyNose only\ntemp_mod_simple <- linear_mod |>\n  fit(BodyTemp ~ RunnyNose, data = dat)\n\n# Model 2: all predictors\ntemp_mod_all <- linear_mod |>\n  fit(BodyTemp ~ ., data = dat)\n\nWait, that is it? Yes, indeed, now our two models are fitted. We may want to manually take a look at them which is quite easy.\n\nprint(temp_mod_simple)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ RunnyNose, data = data)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n     99.1431       -0.2926  \n\n\nOr we can get more information by extracting the engine fit, which will then allow base R to recognize the object.\n\ntemp_mod_simple |>\n    parsnip::extract_fit_engine() |>\n    summary()\n\n\nCall:\nstats::lm(formula = BodyTemp ~ RunnyNose, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9431 -0.7505 -0.3505  0.3495  4.1495 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)  99.14313    0.08191 1210.426  < 2e-16 ***\nRunnyNoseYes -0.29265    0.09714   -3.013  0.00268 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.19 on 728 degrees of freedom\nMultiple R-squared:  0.01231,   Adjusted R-squared:  0.01096 \nF-statistic: 9.076 on 1 and 728 DF,  p-value: 0.00268\n\n\nI know Andreas said no p-values, but it‚Äôs very difficult to tell this function not to show them!\nThis basically works the same for any glm() type object and for most things that come from base R (well, from stats, not base, but it‚Äôs automatically loaded for you either way). We can do the same thing for the other model.\n\nprint(temp_mod_all)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ ., data = data)\n\nCoefficients:\n         (Intercept)  SwollenLymphNodesYes    ChestCongestionYes  \n           98.047181             -0.165302              0.087326  \n     ChillsSweatsYes    NasalCongestionYes            CoughYNYes  \n            0.201266             -0.215771              0.313893  \n           SneezeYes            FatigueYes    SubjectiveFeverYes  \n           -0.361924              0.264762              0.436837  \n         HeadacheYes            Weakness.L            Weakness.Q  \n            0.011453              0.268556              0.128131  \n          Weakness.C         WeaknessYNYes               Cough.L  \n            0.029358                    NA             -0.057709  \n             Cough.Q               Cough.C           CoughYN2Yes  \n           -0.030385              0.089783                    NA  \n           Myalgia.L             Myalgia.Q             Myalgia.C  \n           -0.128819             -0.134721              0.097416  \n        MyalgiaYNYes          RunnyNoseYes             AbPainYes  \n                  NA             -0.080485              0.031574  \n        ChestPainYes           DiarrheaYes              EyePnYes  \n            0.105071             -0.156806              0.131544  \n         InsomniaYes           ItchyEyeYes             NauseaYes  \n           -0.006824             -0.008016             -0.034066  \n            EarPnYes            HearingYes        PharyngitisYes  \n            0.093790              0.232203              0.317581  \n       BreathlessYes            ToothPnYes             VisionYes  \n            0.090526             -0.022876             -0.274625  \n            VomitYes             WheezeYes  \n            0.165272             -0.046665  \n\ntemp_mod_all |>\n    parsnip::extract_fit_engine() |>\n    summary()\n\n\nCall:\nstats::lm(formula = BodyTemp ~ ., data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2110 -0.7219 -0.2853  0.4342  4.2095 \n\nCoefficients: (3 not defined because of singularities)\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          98.047181   0.294443 332.992  < 2e-16 ***\nSwollenLymphNodesYes -0.165302   0.091959  -1.798 0.072682 .  \nChestCongestionYes    0.087326   0.097546   0.895 0.370973    \nChillsSweatsYes       0.201266   0.127302   1.581 0.114330    \nNasalCongestionYes   -0.215771   0.113798  -1.896 0.058362 .  \nCoughYNYes            0.313893   0.240738   1.304 0.192707    \nSneezeYes            -0.361924   0.098299  -3.682 0.000249 ***\nFatigueYes            0.264762   0.160558   1.649 0.099596 .  \nSubjectiveFeverYes    0.436837   0.103398   4.225 2.71e-05 ***\nHeadacheYes           0.011453   0.125405   0.091 0.927256    \nWeakness.L            0.268556   0.162229   1.655 0.098291 .  \nWeakness.Q            0.128131   0.115371   1.111 0.267126    \nWeakness.C            0.029358   0.082041   0.358 0.720573    \nWeaknessYNYes               NA         NA      NA       NA    \nCough.L              -0.057709   0.220131  -0.262 0.793279    \nCough.Q              -0.030385   0.150408  -0.202 0.839964    \nCough.C               0.089783   0.091916   0.977 0.329011    \nCoughYN2Yes                 NA         NA      NA       NA    \nMyalgia.L            -0.128819   0.147101  -0.876 0.381487    \nMyalgia.Q            -0.134721   0.105353  -1.279 0.201410    \nMyalgia.C             0.097416   0.080760   1.206 0.228139    \nMyalgiaYNYes                NA         NA      NA       NA    \nRunnyNoseYes         -0.080485   0.108526  -0.742 0.458569    \nAbPainYes             0.031574   0.140236   0.225 0.821927    \nChestPainYes          0.105071   0.106980   0.982 0.326365    \nDiarrheaYes          -0.156806   0.129545  -1.210 0.226522    \nEyePnYes              0.131544   0.129757   1.014 0.311047    \nInsomniaYes          -0.006824   0.090797  -0.075 0.940114    \nItchyEyeYes          -0.008016   0.110191  -0.073 0.942028    \nNauseaYes            -0.034066   0.102049  -0.334 0.738620    \nEarPnYes              0.093790   0.113875   0.824 0.410436    \nHearingYes            0.232203   0.222043   1.046 0.296037    \nPharyngitisYes        0.317581   0.121342   2.617 0.009057 ** \nBreathlessYes         0.090526   0.099837   0.907 0.364863    \nToothPnYes           -0.022876   0.113750  -0.201 0.840673    \nVisionYes            -0.274625   0.277681  -0.989 0.323010    \nVomitYes              0.165272   0.151432   1.091 0.275478    \nWheezeYes            -0.046665   0.107036  -0.436 0.662990    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.144 on 695 degrees of freedom\nMultiple R-squared:  0.1287,    Adjusted R-squared:  0.08605 \nF-statistic: 3.019 on 34 and 695 DF,  p-value: 4.197e-08\n\n\nHere you can see the ‚Äú3 not defined because of singularities‚Äù message in the coefficients section. I‚Äôll explain this in a bit.\n\n\n\n\n\n\nNote\n\n\n\nYou can also see an ‚ÄúL‚Äù, ‚ÄúQ‚Äù, and ‚ÄúC‚Äù after the names of the ordinal predictors. That is because in R, ordinal variables are parametrized as orthogonal polynomials by default: basically the levels are coded as 1, 2, 3, 4 (in the order none, weak, moderate, severe if you specified the coding correctly) and this numerical variable is included in the model as a linear effect (L), as a quadratic effect (Q, aka squared), and as a cubic effect (C, aka cubed). The orthogonal part just means that these variables are constructed in a way that the L, Q, and C effect are independent of each other.\n\n\nOK, so now let‚Äôs get into the part of comparing the model fits. If you‚Äôve taken BIOS 8010 or 8020 in our department, you are probably most familiar with something like the following.\n\nlm_list <-\n    list(temp_mod_simple, temp_mod_all) |>\n    purrr::map(parsnip::extract_fit_engine)\nrlang::exec(anova, !!!lm_list, test = NULL)\n\nAnalysis of Variance Table\n\nModel 1: BodyTemp ~ RunnyNose\nModel 2: BodyTemp ~ SwollenLymphNodes + ChestCongestion + ChillsSweats + \n    NasalCongestion + CoughYN + Sneeze + Fatigue + SubjectiveFever + \n    Headache + Weakness + WeaknessYN + Cough + CoughYN2 + Myalgia + \n    MyalgiaYN + RunnyNose + AbPain + ChestPain + Diarrhea + EyePn + \n    Insomnia + ItchyEye + Nausea + EarPn + Hearing + Pharyngitis + \n    Breathless + ToothPn + Vision + Vomit + Wheeze\n  Res.Df     RSS Df Sum of Sq\n1    728 1030.53             \n2    695  909.12 33    121.41\n\n\n\n\n\n\n\n\nWhat the heck is that code\n\n\n\n\n\nSo of course you could do the same thing as above by doing something like\nanova(parsnip::extract_fit_engine(temp_mod_simple), parsnip::extract_fit_engine(temp_mod_all))\nbut I suspect that doesn‚Äôt even fit on the screen correctly. This is just a way that will allow you to do that for a lot of lm models at one time, if you need to. If you want to read about the details, you can look at the rlang documentation, but it isn‚Äôt necessary for this class.\n\n\n\nAnd then based on this ANOVA table, you would either do a statistical test to determine if the reduction in the sum of squares is significant or whatever. Here I didn‚Äôt show the p-value because Andreas and I both think that you should decide on your own whether that reduction in sum of squares is meaningful for your problem. However, that can be kind of hard because we aren‚Äôt used to think about squared units. So instead look at some other metrics that are pretty easy to get.\n\n\n\n\n\n\nRank-deficiency\n\n\n\nNote that depending on what you tried to do here, you might have gotten a warning that says something about ‚Äúrank deficiency‚Äù. This is caused by the same issue that gave us 3 coefficients ‚Äúnot defined because of singularities‚Äù. this is the same thing. Our cough, myalgia, and weakness yes/no variables do not provide any information that is different from what‚Äôs in the severity variables (once you know someone‚Äôs severity, you automatically know their response to the yes/no variable). Our model doesn‚Äôt know what to do in this case, which is why we get these warnings! For now you should ignore them but you shouldn‚Äôt ignore these warnings in real life and we won‚Äôt in the next modeling assignment.\n(If you know some linear algebra or stat theory, this is because our cough, myalgia, and weakness yes/no variables are linearly dependent on the ordinal versions of those variables and thus the coefficients for those variables are not estimable.)\n\n\nSo when we look at these models in terms of RMSE, the difference is not that big, but the \\(R^2\\) is certainly larger, by about 10%. However, you may recall from your stat classes that this \\(R^2\\) is not adjusted and thus will always increased when we add more predictors. Since we are building predictive models, that doesn‚Äôt typically matter as much as it does when we‚Äôre trying to build inferential models. But you should certainly notice that the changes in RMSE and MAE are both much smaller in magnitude, indicating that the model with more predictors doesn‚Äôt do that much better of a job at prediction.\nWe‚Äôll use the broom::glance() function to get metrics now with a more in-depth discussion of model evaluation in the next model.\n\ntemp_simple_res <- broom::glance(temp_mod_simple)\ntemp_all_res <- broom::glance(temp_mod_all)\n\ntemp_simple_res\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0123        0.0110  1.19      9.08 0.00268     1 -1162. 2329. 2343.\n# ‚Ñπ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\ntemp_all_res\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.129        0.0860  1.14      3.02 0.0000000420    34 -1116. 2304. 2469.\n# ‚Ñπ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\nAs you can see, we get the unadjusted and adjusted \\(R^2\\), and it seems that the model with more predictors does fit a bit better. Although this is also an interesting case where the AIC and BIC can disagree with each other on the best model (this happens most often when you are comparing models with a large difference in the number of parameters).\nOK, now let‚Äôs move on to the logistic regression models, they are basically the same so you won‚Äôt get as much commentary from me this time.\n\n\nLogistic regression models\nFirst set up the model spec and fit the models.\n\n# Logistic models: Nausea is the outcome of interest\nlogistic_mod <- logistic_reg() |>\n  set_mode(\"classification\") |>\n  set_engine(\"glm\")\n\n# Model 1: RunnyNose only\nnausea_mod_simple <- logistic_mod |>\n  fit(Nausea ~ RunnyNose, data = dat)\n\n# Model 2: all predictors\nnausea_mod_all <- logistic_mod |>\n  fit(Nausea ~ ., data = dat)\n\nNow we‚Äôll peek at the model summaries.\n\nnausea_mod_simple |>\n    parsnip::extract_fit_engine() |>\n    summary()\n\n\nCall:\nstats::glm(formula = Nausea ~ RunnyNose, family = stats::binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9325  -0.9325  -0.9137   1.4439   1.4664  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.65781    0.14520  -4.530 5.89e-06 ***\nRunnyNoseYes  0.05018    0.17182   0.292     0.77    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 944.65  on 729  degrees of freedom\nResidual deviance: 944.57  on 728  degrees of freedom\nAIC: 948.57\n\nNumber of Fisher Scoring iterations: 4\n\nnausea_mod_all |>\n    parsnip::extract_fit_engine() |>\n    summary()\n\n\nCall:\nstats::glm(formula = Nausea ~ ., family = stats::binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9065  -0.8138  -0.5301   0.8581   2.4268  \n\nCoefficients: (3 not defined because of singularities)\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           0.17281    7.84007   0.022 0.982414    \nSwollenLymphNodesYes -0.25108    0.19603  -1.281 0.200248    \nChestCongestionYes    0.27555    0.21266   1.296 0.195066    \nChillsSweatsYes       0.27410    0.28783   0.952 0.340948    \nNasalCongestionYes    0.42582    0.25456   1.673 0.094376 .  \nCoughYNYes           -0.14042    0.51880  -0.271 0.786644    \nSneezeYes             0.17672    0.21035   0.840 0.400828    \nFatigueYes            0.22906    0.37188   0.616 0.537925    \nSubjectiveFeverYes    0.27774    0.22536   1.232 0.217793    \nHeadacheYes           0.33126    0.28490   1.163 0.244937    \nWeakness.L            0.64891    0.35707   1.817 0.069165 .  \nWeakness.Q            0.31697    0.25838   1.227 0.219909    \nWeakness.C           -0.10603    0.18327  -0.579 0.562891    \nWeaknessYNYes              NA         NA      NA       NA    \nCough.L              -0.66937    0.46154  -1.450 0.146977    \nCough.Q              -0.18354    0.31742  -0.578 0.563121    \nCough.C              -0.11737    0.19244  -0.610 0.541928    \nCoughYN2Yes                NA         NA      NA       NA    \nMyalgia.L             0.12772    0.31428   0.406 0.684469    \nMyalgia.Q            -0.03992    0.23003  -0.174 0.862226    \nMyalgia.C            -0.11312    0.17732  -0.638 0.523484    \nMyalgiaYNYes               NA         NA      NA       NA    \nRunnyNoseYes          0.04532    0.23264   0.195 0.845535    \nAbPainYes             0.93930    0.28146   3.337 0.000846 ***\nChestPainYes          0.07078    0.22786   0.311 0.756090    \nDiarrheaYes           1.06393    0.25871   4.113 3.91e-05 ***\nEyePnYes             -0.34199    0.27772  -1.231 0.218164    \nInsomniaYes           0.08418    0.19299   0.436 0.662710    \nItchyEyeYes          -0.06336    0.23250  -0.273 0.785212    \nEarPnYes             -0.18172    0.23921  -0.760 0.447451    \nHearingYes            0.32305    0.45240   0.714 0.475177    \nPharyngitisYes        0.27536    0.26606   1.035 0.300680    \nBreathlessYes         0.52680    0.20858   2.526 0.011548 *  \nToothPnYes            0.48065    0.22947   2.095 0.036209 *  \nVisionYes             0.12550    0.54111   0.232 0.816596    \nVomitYes              2.45847    0.34861   7.052 1.76e-12 ***\nWheezeYes            -0.30443    0.23408  -1.301 0.193417    \nBodyTemp             -0.03125    0.07984  -0.391 0.695526    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 944.65  on 729  degrees of freedom\nResidual deviance: 751.47  on 695  degrees of freedom\nAIC: 821.47\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can again look at the analysis of deviance table (the equivalent to analysis of variance for a non-Gaussian GLM), but again remember that this is just for comparison.\n\nglm_list <-\n    list(nausea_mod_simple, nausea_mod_all) |>\n    purrr::map(parsnip::extract_fit_engine)\nrlang::exec(anova, !!!glm_list, test = NULL)\n\nAnalysis of Deviance Table\n\nModel 1: Nausea ~ RunnyNose\nModel 2: Nausea ~ SwollenLymphNodes + ChestCongestion + ChillsSweats + \n    NasalCongestion + CoughYN + Sneeze + Fatigue + SubjectiveFever + \n    Headache + Weakness + WeaknessYN + Cough + CoughYN2 + Myalgia + \n    MyalgiaYN + RunnyNose + AbPain + ChestPain + Diarrhea + EyePn + \n    Insomnia + ItchyEye + EarPn + Hearing + Pharyngitis + Breathless + \n    ToothPn + Vision + Vomit + Wheeze + BodyTemp\n  Resid. Df Resid. Dev Df Deviance\n1       728     944.57            \n2       695     751.47 33   193.09\n\n\nAnd now let‚Äôs look at the glance() results.\n\nnausea_simple_res <- broom::glance(nausea_mod_simple)\nnausea_all_res <- broom::glance(nausea_mod_all)\n\nnausea_simple_res\n\n# A tibble: 1 √ó 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -472.  949.  958.     945.         728   730\n\nnausea_all_res\n\n# A tibble: 1 √ó 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -376.  821.  982.     751.         695   730\n\n\nThese results look a bit different from the linear regression results ‚Äì here, we can‚Äôt do the \\(R^2\\) in the same way that we can for the linear regression models. The best equivalent metric is the deviance, which is penalized by the number of parameters in the AIC/BIC. Again, we‚Äôll discuss the correct way to interpret model results and goodness-of-fit in the next module, but just know that Andreas and I typically don‚Äôt like the AIC/BIC or the p-value approach.\n\n\nSave results\nOk, finally we‚Äôll save our results. For now I‚Äôll save the glance() results, that way we could use them while writing up our reports. For example, you could include some text like this in your report:\n948.57\nand that lets you easily type things like ‚ÄúThe AIC for the logistic regression model with only runny nose status as a predictor was 948.57, while the AIC for the full model with all predictors was 821.47‚Äù without having to type the numbers ‚Äì check the Quarto document for this page for an example if you want to.\n\nsaveRDS(temp_simple_res, file = here::here(\"fluanalysis\", \"results\", \"temp_simple_res.Rds\"))\nsaveRDS(temp_all_res, file = here::here(\"fluanalysis\", \"results\", \"temp_all_res.Rds\"))\nsaveRDS(nausea_simple_res, file = here::here(\"fluanalysis\", \"results\", \"nausea_simple_res.Rds\"))\nsaveRDS(nausea_all_res, file = here::here(\"fluanalysis\", \"results\", \"nausea_all_res.Rds\"))"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "Flu analysis: Model evaluation",
    "section": "",
    "text": "In this document, we‚Äôll fit some models using more advanced tools from the tidymodels ecosystem, and we‚Äôll dive a bit deeper into model evaluation."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-fitting-with-workflow",
    "href": "fluanalysis/code/modeleval.html#model-fitting-with-workflow",
    "title": "Flu analysis: Model evaluation",
    "section": "Model fitting with workflow",
    "text": "Model fitting with workflow\nFirst we‚Äôll make the model specification, it will be the same as last time.\n\nlogistic_model <- parsnip::logistic_reg() |>\n  parsnip::set_mode(\"classification\") |>\n  parsnip::set_engine(\"glm\")\n\nNext we‚Äôll make a workflow, using R‚Äôs formula syntax.\n\nnausea_all_pred_rec <- recipes::recipe(Nausea ~ ., data = train)\n\nNow we‚Äôll bundle the recipe and the model specification together into a workflow.\n\nnausea_all_pred_wf <- workflows::workflow() |>\n  workflows::add_recipe(nausea_all_pred_rec) |>\n  workflows::add_model(logistic_model)\n\nFortunately, once we have the workflow it is very easy to ‚Äútrain our workflow‚Äù (AKA fit the workflow to our training data).\n\nnausea_all_pred_fit <-\n    nausea_all_pred_wf |>\n    parsnip::fit(data = train)\n\nIf you print out the nausea_all_pred_fit and the nausea_all_pred_wf objects, you can see what gets filled in when the workflow goes from being untrained to being trained."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-evaluation",
    "href": "fluanalysis/code/modeleval.html#model-evaluation",
    "title": "Flu analysis: Model evaluation",
    "section": "Model evaluation",
    "text": "Model evaluation\nOne of the most common metrics for evaluating a logistic regression model is the Area Under the Receiver Operating Characteristic Curve (AUROCC) or ROC AUC (the same acronym but flipped a bit), or just AUC for short. We‚Äôll first evaluate our logistic regression model by examining the ROC curve and its AUC when we use our model to predict the response on the training data.\nFirst we‚Äôll get the predictions on the training data. You might get the rank deficiency error again, that‚Äôs ok! The broom package makes this quite easy, in the old days you used to have to predict() and merge() and it was kind of terrible. But now we can just do this.\n\nnausea_all_pred_aug <-\n  broom::augment(nausea_all_pred_fit, train)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\nNow let‚Äôs look at the ROC curve for our training data. For this plot I‚Äôll also include some example code to save it to a file, but I won‚Äôt do that for the rest because I don‚Äôt need these files again.\n\n# Make the roc curve\nnausea_all_pred_roc <-\n    nausea_all_pred_aug |>\n  yardstick::roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\")\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\n‚Ñπ Please use `reframe()` instead.\n‚Ñπ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n‚Ñπ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n# Print it on this page\nnausea_all_pred_roc |> ggplot2::autoplot()\n\n\n\n# Save it to a file so I can put it in my project later\nggsave(\n    plot = nausea_all_pred_roc |> ggplot2::autoplot(),\n    width = 6.5,\n    height = 6.5,\n    dpi = 300,\n    filename = here::here(\"fluanalysis\", \"results\", \"nausea-all-pred-train-roc.png\")\n)\n\nNow let‚Äôs look at the curve for the test data. Note that our model is fitted to the training data, but we are getting the predictions on the test data. You never want to fit the model to the test data!!!!!!\n\nnausea_all_pred_aug_test <-\n  broom::augment(nausea_all_pred_fit, test)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\nnausea_all_pred_roc_test <-\n    nausea_all_pred_aug_test |>\n  yardstick::roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") |>\n  autoplot()\n\nnausea_all_pred_roc_test\n\n\n\n\nIf you want to plot both of the curves on the same plot, it takes a little bit more finagling, but we can do it. This is probably not the best way to code this, but it is an easy way to do it for sure.\n\ndplyr::bind_rows(\n    \"train\" =   nausea_all_pred_aug |>\n        yardstick::roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\"),\n    \"test\" =    nausea_all_pred_aug_test |>\n        yardstick::roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\"),\n    .id = \"set\"\n) |>\n    ggplot() +\n    aes(\n        x = 1 - specificity,\n        y = sensitivity,\n        color = set\n    ) +\n    # Note that we use geom_path() to keep ggplot2 from automatically sorting\n    # the data, which will make the test curve look weird.\n    geom_path(linewidth = 1.5) +\n    coord_fixed(ratio = 1) +\n    scale_color_manual(values = c(\"dodgerblue\", \"darkorange\")) +\n    theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\n\nWe can also get the numeric AUCs for both data sets.\n\nnausea_all_pred_aug |>\n    yardstick::roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.780\n\nnausea_all_pred_aug_test |>\n    yardstick::roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.747\n\n\nThe two AUCs are pretty similar, which means our model is probably not overfitting by that much, and they are actually OK. This model might be pretty decent."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-comparison",
    "href": "fluanalysis/code/modeleval.html#model-comparison",
    "title": "Flu analysis: Model evaluation",
    "section": "Model comparison",
    "text": "Model comparison\nNext we‚Äôll repeat basically the same steps, but instead of a model with every predictor, we‚Äôll use a model that only has RunnyNose as a predictor. I‚Äôll do most of the code with very little explanation, because you can follow the explanation from the above part.\nHere‚Äôs the fitting part.\n\n# Recipe with only runny nose\nnausea_rn_rec <- recipes::recipe(Nausea ~ RunnyNose, data = train)\n\n# Workflow with only runny nose\nnausea_rn_wf <-\n    workflows::workflow() |>\n  workflows::add_recipe(nausea_rn_rec) |>\n  workflows::add_model(logistic_model)\n\n# Fit model with only runny nose\nnausea_rn_fit <-\n    nausea_rn_wf |>\n    parsnip::fit(data = train)\n\nNow we evaluate the fit on the training data.\n\n# Examine ROC on training data\nnausea_rn_aug <- broom::augment(nausea_rn_fit, train)\n\nnausea_rn_roc <-\n    nausea_rn_aug |>\n  yardstick::roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") |>\n  autoplot()\nnausea_rn_roc\n\n\n\nnausea_rn_auc <-\n    nausea_rn_aug |>\n  yardstick::roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\nnausea_rn_auc\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.508\n\n\nAnd finally we evaluate the fit on the testing data.\n\n# Examine ROC on testing data\nnausea_rn_aug_test <- broom::augment(nausea_rn_fit, test)\n\nnausea_rn_roc_test <-\n    nausea_rn_aug_test |>\n  yardstick::roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") |>\n  autoplot()\nnausea_rn_roc_test\n\n\n\nnausea_rn_auc_test <-\n    nausea_rn_aug_test |>\n  yardstick::roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\nnausea_rn_auc_test\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.497\n\n\nTo be honest, those are some of the worst ROCs I‚Äôve ever seen! It is clear that runny nose status does not predict nausea very well, and the predictive power that we saw before must be coming from somewhere else. Note that our ROC curve here looks mostly like a straight line ‚Äì since we only have one dichotomous predictor, our model is only capable of giving us two discrete outputs. We get one predicted probability of nausea when there is no runny nose, and one for patients who do have a runny nose. Since we only get two probabilities, our ROC curve only has three points that define it: we assign everyone no (0% sensitivity, 100% specificity), we assign everyone yes (100% sensitivity, 0% specificity), or we assign one probability to yes and the other to no. So if this predictor were a bit more predictive, we would see a sort of angle shape for our ROC curve where it bends exactly once. Our ROC curve was really bendy in the full model cause there were a lot of these probability cutoffs to evaluate.\nOK, that‚Äôs all I‚Äôll say about that and now we are on to the continuous outcome models."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-fitting-with-workflow-1",
    "href": "fluanalysis/code/modeleval.html#model-fitting-with-workflow-1",
    "title": "Flu analysis: Model evaluation",
    "section": "Model fitting with workflow",
    "text": "Model fitting with workflow\n\nlm_mod <-\n    parsnip::linear_reg() |>\n    parsnip::set_engine(\"lm\") |>\n    parsnip::set_mode(\"regression\")\n\n# BodyTemp is the outcome, use all predictors\nlm_rec_all <- recipes::recipe(BodyTemp ~ ., data = train)\n\nlm_wf_all <-\n    workflows::workflow() |>\n    workflows::add_recipe(lm_rec_all) |>\n    workflows::add_model(lm_mod)\n\n# Fit the model\nlm_fit_all <-\n    lm_wf_all |>\n    parsnip::fit(data = train)\n\nlm_fit_all_train <- broom::augment(lm_fit_all, train)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\nlm_fit_all_test <- broom::augment(lm_fit_all, test)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\nUnfortunately we don‚Äôt get any pretty ROC plots for our continuous outcome. (Well, I guess we could, but we would have to dichotomize the outcome and that is something we encourage you all NOT to do!!) In the next exercise we‚Äôll talk more about how to visualize these kinds of results, for now we‚Äôll just calculate the RMSE.\n\nlm_all_rmse_train <-\n    lm_fit_all_train |>\n    yardstick::rmse(truth = BodyTemp, estimate = .pred)\nlm_all_rmse_test <-\n    lm_fit_all_test |>\n    yardstick::rmse(truth = BodyTemp, estimate = .pred)\n\nlm_all_rmse_train\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.09\n\nlm_all_rmse_test\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.21\n\n\nOur RMSE is a bit higher on the test data than it is on the training data, which we expect. Unlike the ROC AUC though, this is not a unitless measure of performance! Of course if our model were perfect, it would be 0, but there is no absolute maximum and we have to interpret this in the context of the units of the outcome. I think it is pretty fair to say that 1.09 temperature degrees (Fahrenheit) is not that much less than 1.21, it is about 0.1 degrees less, which was the degree of precision to which our measurements were taken anyways. If we really wanted a basis of comparison for these, we would also fit a null model, which we‚Äôll talk about in the next exercise as well. We could also use a metric like \\(R^2\\) which has an absolute range.\nOK, now let‚Äôs move on to fitting the reduced model."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-evaluation-and-comparison.",
    "href": "fluanalysis/code/modeleval.html#model-evaluation-and-comparison.",
    "title": "Flu analysis: Model evaluation",
    "section": "Model evaluation and comparison.",
    "text": "Model evaluation and comparison.\nNow we‚Äôll fit a model that only has RunnyNose as a predictor and do the same steps. Again, this works exactly the same as the previous example so I‚Äôm skipping a lot of the details.\n\n# Set up a workflow where RunnyNose is the only predictor\nlm_rec_rn <- recipes::recipe(BodyTemp ~ RunnyNose, data = train)\nlm_wf_rn <-\n    workflows::workflow() |>\n    workflows::add_recipe(lm_rec_rn) |>\n    workflows::add_model(lm_mod)\n\n# Fit the model\nlm_fit_rn <-\n    lm_wf_rn |>\n    parsnip::fit(data = train)\n\nlm_fit_rn_train <- broom::augment(lm_fit_rn, train)\nlm_fit_rn_test <- broom::augment(lm_fit_rn, test)\n\nThat‚Äôs really all it takes to get the models, and getting the RMSE is just as easy.\n\nlm_rn_rmse_train <-\n    lm_fit_rn_train |>\n    yardstick::rmse(truth = BodyTemp, estimate = .pred)\nlm_rn_rmse_test <-\n    lm_fit_rn_test |>\n    yardstick::rmse(truth = BodyTemp, estimate = .pred)\n\nlm_rn_rmse_train\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.17\n\nlm_rn_rmse_test\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.23\n\n\nBoth of these models are worse than the full models, but like I said before, we have to interpret these in the units of the response (BodyTemp). So when we do that, I think we can say that this model is not that much worse. Which likely means our predictors are just not going a great job of modeling BodyTemp at all, in real life we would definitely want to do the null model comparison to see what is going on here."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "Flu analysis wrangling",
    "section": "",
    "text": "For this analysis, we‚Äôll look at data from Brian McKay et al.‚Äôs paper, Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of patients infected with influenza. You can find the data here.\nFirst we‚Äôll read in his processed data file.\n\n# Load the data\ndat_orig <-\n    readr::read_rds(\n        here::here(\"fluanalysis\", \"data\", \"SympAct_Any_Pos.Rda\")\n    )\n\nWe need to do a little bit of data cleaning to get this ready for our secondary (‚Ä¶tertiary?) analysis. We only want some of the variables, and for this project we‚Äôll just drop any missing observations, although in real life we would probably want to think about it a little bit more before dropping the missing observations (that is a joke, but it seems to be what everyone means when they say ‚Äúconsider missing data more closely‚Äù).\n\ndat <- dat_orig |>\n    # Remove the unwanted variables\n    dplyr::select(\n        -tidyselect::matches(\"(Score|Total|FluA|FluB|Dxname|Activity)\"),\n        -Unique.Visit\n    ) |>\n    # Rename CoughIntensity to Cough to save me 10 seconds of typing\n    dplyr::rename(Cough = CoughIntensity) |>\n    # Code symptom intensities as ordered factors\n    dplyr::mutate(\n        dplyr::across(\n            .cols = c(Cough, Myalgia, Weakness),\n            .fns = ~factor(.x,\n                                         levels = c(\"None\", \"Mild\", \"Moderate\", \"Severe\"),\n                                         ordered = TRUE)\n        )\n    ) |>\n    # Drop rows with any missing variables\n    tidyr::drop_na()\n\n# Check to make sure the data is right\ndplyr::glimpse(dat)\n\nRows: 730\nColumns: 32\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y‚Ä¶\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, ‚Ä¶\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, ‚Ä¶\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, ‚Ä¶\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes‚Ä¶\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes‚Ä¶\n$ Weakness          <ord> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi‚Ä¶\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ Cough             <ord> Severe, Severe, Mild, Moderate, None, Moderate, Seve‚Ä¶\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes‚Ä¶\n$ Myalgia           <ord> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, ‚Ä¶\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No‚Ä¶\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N‚Ä¶\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, ‚Ä¶\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,‚Ä¶\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye‚Ä¶\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,‚Ä¶\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y‚Ä¶\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y‚Ä¶\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,‚Ä¶\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, ‚Ä¶\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, ‚Ä¶\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N‚Ä¶\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, ‚Ä¶\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N‚Ä¶\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N‚Ä¶\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, ‚Ä¶\n\n\nWe have 730 observations of 32 variables, just like we are supposed to. So we can move on to the EDA step after we save the data.\n\nreadr::write_rds(\n    dat,\n    here::here(\"fluanalysis\", \"data\", \"clean-data.Rds\")\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Hi everyone! My name is Zane and I‚Äôll be the TA for MADA this semester. You can read some more about me on my About Me page!\nI‚Äôll host live/synchronous Zoom office hours once a week, similar to Andreas. My meetings will be\n\nWednesday at 1:00 pm for odd-numbered modules and\nThursday at 1:00 pm for even-numbered modules at\nhttps://zoom.us/my/wzbillings.\n\nSo hopefully between my office hour and Andreas‚Äô office hour, there will be a time that works for you. My first office hour will be Wednesday, January 11th, at 1:00 pm.\nFeel free to DM me in our slack group, or you can email me at Wesley.Billings@uga.edu. Though be advised that I never check my phone, so I am unlikely to see emails or slack messages outside of ‚Äúregular business hours‚Äù."
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "Placeholder file for the future visualization exercise."
  }
]